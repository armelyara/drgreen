{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRDhfR_pqRbO"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/armelyara/drgreen/blob/Drgreen_V2/drgreen_v4_improved.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uO7W9a1QqRbP"
      },
      "source": [
        "# üåø Dr Green V4 - Improved Medicinal Plant Recognition\n",
        "\n",
        "**Enhanced version with better accuracy through advanced techniques**\n",
        "\n",
        "### Improvements over V3:\n",
        "- Two-phase training: Frozen base ‚Üí Fine-tuning\n",
        "- Advanced data augmentation (MixUp, CutMix)\n",
        "- Test-Time Augmentation (TTA) for predictions\n",
        "- Better learning rate scheduling\n",
        "- Focal loss for handling class imbalance\n",
        "- EfficientNetB3 for better feature extraction\n",
        "\n",
        "### Target: 90%+ accuracy\n",
        "\n",
        "### Plant Classes:\n",
        "1. Artemisia (Armoise) - Antimalarial\n",
        "2. Carica (Papaya) - Digestive aid\n",
        "3. Goyavier (Guava) - Antiseptic\n",
        "4. Kinkeliba - Detoxifying"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkLQO_ciqRbQ"
      },
      "source": [
        "## 1. Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0k_LWHMcqRbQ"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q gdown\n",
        "\n",
        "# Core imports\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import json\n",
        "from datetime import datetime\n",
        "from PIL import Image\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
        "print(f\"Keras backend: {tf.keras.backend.backend()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SvbrPf8eqRbQ"
      },
      "outputs": [],
      "source": [
        "# Download dataset from Google Drive\n",
        "import gdown\n",
        "\n",
        "# Google Drive file ID\n",
        "file_id = '1zI5KfTtuV0BlBQnNDNq4tBJuEkxLZZBD'\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "output = '/content/drgreen.zip'\n",
        "\n",
        "print(\"üì• Downloading dataset from Google Drive...\")\n",
        "gdown.download(url, output, quiet=False)\n",
        "print(\"‚úì Dataset downloaded successfully!\")\n",
        "\n",
        "# Extract dataset\n",
        "extract_dir = '/content'\n",
        "print(\"\\nüìÇ Extracting dataset...\")\n",
        "with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_dir)\n",
        "print(f\"‚úì Dataset extracted to '{extract_dir}'\")\n",
        "\n",
        "# List extracted contents\n",
        "print(\"\\nüìã Dataset contents:\")\n",
        "for item in os.listdir(extract_dir):\n",
        "    if os.path.isdir(os.path.join(extract_dir, item)) and item not in ['sample_data', '.config']:\n",
        "        print(f\"  - {item}/\")\n",
        "        # List subdirectories (plant classes)\n",
        "        item_path = os.path.join(extract_dir, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            for subitem in os.listdir(item_path):\n",
        "                subitem_path = os.path.join(item_path, subitem)\n",
        "                if os.path.isdir(subitem_path):\n",
        "                    num_files = len([f for f in os.listdir(subitem_path) if os.path.isfile(os.path.join(subitem_path, f))])\n",
        "                    print(f\"      {subitem}/ ({num_files} images)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkVIiSk3qRbR"
      },
      "source": [
        "## 2. Enhanced Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ApoyJcmqRbR"
      },
      "outputs": [],
      "source": [
        "# Enhanced configuration\n",
        "CONFIG = {\n",
        "    # Paths\n",
        "    'data_dir': 'rename',\n",
        "    'model_save_dir': 'models',\n",
        "\n",
        "    # Image parameters\n",
        "    'img_height': 300,  # Increased from 224\n",
        "    'img_width': 300,\n",
        "    'batch_size': 16,  # Reduced for better generalization\n",
        "\n",
        "    # Training parameters - Phase 1 (Frozen base)\n",
        "    'phase1_epochs': 30,\n",
        "    'phase1_lr': 1e-2,\n",
        "\n",
        "    # Training parameters - Phase 2 (Fine-tuning)\n",
        "    'phase2_epochs': 20,\n",
        "    'phase2_lr': 1e-4,\n",
        "    'unfreeze_from_layer': 100,  # Unfreeze last N layers\n",
        "\n",
        "    'validation_split': 0.2,\n",
        "    'seed': 42,\n",
        "\n",
        "    # Model parameters\n",
        "    'base_model': 'EfficientNetB3',  # Upgraded from B0\n",
        "    'dropout_rate': 0.5,\n",
        "    'num_classes': 4,\n",
        "    'dense_units': 256,\n",
        "\n",
        "    # Advanced techniques\n",
        "    'use_mixup': True,\n",
        "    'mixup_alpha': 0.2,\n",
        "    'use_tta': True,  # Test-time augmentation\n",
        "    'tta_steps': 5,\n",
        "\n",
        "    # Callbacks\n",
        "    'early_stopping_patience': 10,\n",
        "    'reduce_lr_patience': 5,\n",
        "    'reduce_lr_factor': 0.5,\n",
        "    'min_lr': 1e-8,\n",
        "}\n",
        "\n",
        "PLANT_CLASSES = ['artemisia', 'carica', 'goyavier', 'kinkeliba']\n",
        "Path(CONFIG['model_save_dir']).mkdir(exist_ok=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DR GREEN V4 - IMPROVED CONFIGURATION\")\n",
        "print(\"=\"*60)\n",
        "print(json.dumps(CONFIG, indent=2))\n",
        "print(\"\\n‚úì Two-phase training strategy\")\n",
        "print(\"‚úì Advanced data augmentation\")\n",
        "print(\"‚úì Test-time augmentation\")\n",
        "print(\"‚úì Upgraded to EfficientNetB3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E4W6xGVqRbR"
      },
      "source": [
        "## 3. Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjXcEmsoqRbR"
      },
      "outputs": [],
      "source": [
        "# Load dataset with train/validation split\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    CONFIG['data_dir'],\n",
        "    validation_split=CONFIG['validation_split'],\n",
        "    subset=\"training\",\n",
        "    seed=CONFIG['seed'],\n",
        "    image_size=(CONFIG['img_height'], CONFIG['img_width']),\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    label_mode='int'\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    CONFIG['data_dir'],\n",
        "    validation_split=CONFIG['validation_split'],\n",
        "    subset=\"validation\",\n",
        "    seed=CONFIG['seed'],\n",
        "    image_size=(CONFIG['img_height'], CONFIG['img_width']),\n",
        "    batch_size=CONFIG['batch_size'],\n",
        "    label_mode='int'\n",
        ")\n",
        "\n",
        "class_names = train_ds.class_names\n",
        "print(f\"\\nClasses found: {class_names}\")\n",
        "print(f\"Number of classes: {len(class_names)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g63OrR9qRbR"
      },
      "outputs": [],
      "source": [
        "# Calculate dataset statistics\n",
        "train_batches = tf.data.experimental.cardinality(train_ds).numpy()\n",
        "val_batches = tf.data.experimental.cardinality(val_ds).numpy()\n",
        "\n",
        "print(f\"\\nDataset Statistics:\")\n",
        "print(f\"Training batches: {train_batches}\")\n",
        "print(f\"Validation batches: {val_batches}\")\n",
        "print(f\"Batch size: {CONFIG['batch_size']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SHAeHTVqRbR"
      },
      "source": [
        "## 4. Calculate Class Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wU9v775vqRbS"
      },
      "outputs": [],
      "source": [
        "# Count samples per class\n",
        "class_counts = {name: 0 for name in class_names}\n",
        "\n",
        "for images, labels in train_ds:\n",
        "    for label in labels.numpy():\n",
        "        class_counts[class_names[label]] += 1\n",
        "\n",
        "for images, labels in val_ds:\n",
        "    for label in labels.numpy():\n",
        "        class_counts[class_names[label]] += 1\n",
        "\n",
        "# Calculate class weights\n",
        "total_samples = sum(class_counts.values())\n",
        "class_weights = {}\n",
        "\n",
        "for i, class_name in enumerate(class_names):\n",
        "    class_weights[i] = total_samples / (len(class_names) * class_counts[class_name])\n",
        "\n",
        "print(\"\\nClass distribution:\")\n",
        "for class_name, count in class_counts.items():\n",
        "    print(f\"  {class_name}: {count} images\")\n",
        "\n",
        "print(\"\\nClass weights:\")\n",
        "for i, weight in class_weights.items():\n",
        "    print(f\"  {class_names[i]:12s}: {weight:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjaJbx_5qRbS"
      },
      "source": [
        "## 5. Advanced Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ee2Q05gHqRbS"
      },
      "outputs": [],
      "source": [
        "# Strong data augmentation pipeline\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    tf.keras.layers.RandomRotation(0.4),\n",
        "    tf.keras.layers.RandomZoom(0.3),\n",
        "    tf.keras.layers.RandomBrightness(0.3),\n",
        "    tf.keras.layers.RandomContrast(0.3),\n",
        "    tf.keras.layers.RandomTranslation(0.2, 0.2),\n",
        "], name=\"data_augmentation\")\n",
        "\n",
        "# Preprocessing for EfficientNet\n",
        "preprocess_input = tf.keras.applications.efficientnet.preprocess_input\n",
        "\n",
        "print(\"Enhanced data augmentation layers:\")\n",
        "for i, layer in enumerate(data_augmentation.layers, 1):\n",
        "    print(f\"  {i}. {layer.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PqYKp0BHqRbS"
      },
      "source": [
        "## 6. MixUp Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COUC9qLNqRbS"
      },
      "outputs": [],
      "source": [
        "def mixup(images, labels, alpha=0.2):\n",
        "    \"\"\"\n",
        "    MixUp augmentation: creates virtual training examples\n",
        "    by mixing pairs of examples and their labels\n",
        "    \"\"\"\n",
        "    batch_size = tf.shape(images)[0]\n",
        "\n",
        "    # Sample lambda from Beta distribution\n",
        "    lambda_val = tf.random.uniform([], 0, alpha)\n",
        "\n",
        "    # Create random indices for mixing\n",
        "    indices = tf.random.shuffle(tf.range(batch_size))\n",
        "\n",
        "    # Mix images\n",
        "    mixed_images = lambda_val * images + (1 - lambda_val) * tf.gather(images, indices)\n",
        "\n",
        "    # Convert labels to one-hot\n",
        "    labels_one_hot = tf.one_hot(labels, CONFIG['num_classes'])\n",
        "    mixed_labels_one_hot = lambda_val * labels_one_hot + (1 - lambda_val) * tf.gather(labels_one_hot, indices)\n",
        "\n",
        "    return mixed_images, mixed_labels_one_hot\n",
        "\n",
        "print(\"‚úì MixUp augmentation function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkJPft-9qRbS"
      },
      "source": [
        "## 7. Optimize Dataset Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8QriEuIqRbS"
      },
      "outputs": [],
      "source": [
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "# Prepare training dataset with augmentation\n",
        "def prepare_train(x, y):\n",
        "    x = data_augmentation(x, training=True)\n",
        "    x = preprocess_input(x)\n",
        "    return x, y\n",
        "\n",
        "def prepare_val(x, y):\n",
        "    x = preprocess_input(x)\n",
        "    return x, y\n",
        "\n",
        "train_ds_prepared = train_ds.map(prepare_train, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "# Apply MixUp if enabled\n",
        "if CONFIG['use_mixup']:\n",
        "    train_ds_prepared = train_ds_prepared.map(\n",
        "        lambda x, y: mixup(x, y, CONFIG['mixup_alpha']),\n",
        "        num_parallel_calls=AUTOTUNE\n",
        "    )\n",
        "    print(\"‚úì MixUp enabled for training\")\n",
        "\n",
        "train_ds_prepared = train_ds_prepared.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "val_ds_prepared = val_ds.map(prepare_val, num_parallel_calls=AUTOTUNE)\n",
        "val_ds_prepared = val_ds_prepared.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "print(\"Dataset pipeline optimized\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7biU05J_qRbS"
      },
      "source": [
        "## 8. Build Enhanced Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1VjwpF1qRbS"
      },
      "outputs": [],
      "source": [
        "def build_model(num_classes, dropout_rate, dense_units):\n",
        "    \"\"\"\n",
        "    Build EfficientNetB3 model with enhanced architecture\n",
        "    \"\"\"\n",
        "    inputs = tf.keras.Input(shape=(CONFIG['img_height'], CONFIG['img_width'], 3))\n",
        "\n",
        "    # Load EfficientNetB3 pretrained base\n",
        "    base_model = tf.keras.applications.EfficientNetB3(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_tensor=inputs,\n",
        "        pooling='avg'\n",
        "    )\n",
        "\n",
        "    # Initially freeze base model\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # Enhanced classification head\n",
        "    x = base_model.output\n",
        "\n",
        "    # First dense block\n",
        "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "    x = tf.keras.layers.Dense(\n",
        "        dense_units,\n",
        "        activation='relu',\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
        "    )(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    # Second dense block\n",
        "    x = tf.keras.layers.Dropout(dropout_rate / 2)(x)\n",
        "    x = tf.keras.layers.Dense(\n",
        "        dense_units // 2,\n",
        "        activation='relu',\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(0.01)\n",
        "    )(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    # Output layer\n",
        "    x = tf.keras.layers.Dropout(dropout_rate / 4)(x)\n",
        "    outputs = tf.keras.layers.Dense(\n",
        "        num_classes,\n",
        "        activation='softmax',\n",
        "        dtype=tf.float32\n",
        "    )(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='DrGreen_v4_Improved')\n",
        "\n",
        "    return model, base_model\n",
        "\n",
        "# Build model\n",
        "model, base_model = build_model(\n",
        "    num_classes=CONFIG['num_classes'],\n",
        "    dropout_rate=CONFIG['dropout_rate'],\n",
        "    dense_units=CONFIG['dense_units']\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MODEL ARCHITECTURE\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Base: {CONFIG['base_model']} (initially frozen)\")\n",
        "print(f\"Hidden units: {CONFIG['dense_units']}\")\n",
        "print(f\"Dropout rate: {CONFIG['dropout_rate']}\")\n",
        "print(f\"Total parameters: {model.count_params():,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZLOu66WqRbS"
      },
      "source": [
        "## 9. Focal Loss (Better for Imbalanced Data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvo87I_PqRbT"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(tf.keras.losses.Loss):\n",
        "    \"\"\"\n",
        "    Focal Loss: focuses training on hard examples\n",
        "    Better than cross-entropy for imbalanced datasets\n",
        "    \"\"\"\n",
        "    def __init__(self, gamma=2.0, alpha=0.25, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        # Clip predictions to prevent log(0)\n",
        "        y_pred = tf.clip_by_value(y_pred, 1e-7, 1.0 - 1e-7)\n",
        "\n",
        "        # Calculate cross entropy\n",
        "        ce = -y_true * tf.math.log(y_pred)\n",
        "\n",
        "        # Calculate focal weight\n",
        "        weight = self.alpha * y_true * tf.pow(1 - y_pred, self.gamma)\n",
        "\n",
        "        # Apply focal weight\n",
        "        focal_loss = weight * ce\n",
        "\n",
        "        return tf.reduce_sum(focal_loss, axis=-1)\n",
        "\n",
        "print(\"‚úì Focal Loss implemented\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIPiIDDWqRbT"
      },
      "source": [
        "## 10. Phase 1 Training: Frozen Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQr_TdcWqRbT"
      },
      "outputs": [],
      "source": [
        "# Compile for phase 1\n",
        "loss_fn = FocalLoss(gamma=2.0, alpha=0.25) if CONFIG['use_mixup'] else tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=CONFIG['phase1_lr']),\n",
        "    loss=loss_fn,\n",
        "    metrics=[\n",
        "        tf.keras.metrics.CategoricalAccuracy(name='accuracy') if CONFIG['use_mixup']\n",
        "        else tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy'),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Phase 1 callbacks\n",
        "phase1_callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=CONFIG['early_stopping_patience'],\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=CONFIG['reduce_lr_factor'],\n",
        "        patience=CONFIG['reduce_lr_patience'],\n",
        "        min_lr=CONFIG['min_lr'],\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=f\"{CONFIG['model_save_dir']}/phase1_best.keras\",\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 1: Training with frozen base\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Epochs: {CONFIG['phase1_epochs']}\")\n",
        "print(f\"Learning rate: {CONFIG['phase1_lr']}\")\n",
        "print(f\"Base trainable: {base_model.trainable}\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Train phase 1\n",
        "history_phase1 = model.fit(\n",
        "    train_ds_prepared,\n",
        "    validation_data=val_ds_prepared,\n",
        "    epochs=CONFIG['phase1_epochs'],\n",
        "    callbacks=phase1_callbacks,\n",
        "    class_weight=class_weights if not CONFIG['use_mixup'] else None,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n‚úì Phase 1 training completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV0Rki8pqRbT"
      },
      "source": [
        "## 11. Phase 2 Training: Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOtqtq3fqRbT"
      },
      "outputs": [],
      "source": [
        "# Unfreeze top layers of base model\n",
        "base_model.trainable = True\n",
        "\n",
        "# Freeze all layers except the last N\n",
        "for layer in base_model.layers[:-CONFIG['unfreeze_from_layer']]:\n",
        "    layer.trainable = False\n",
        "\n",
        "trainable_layers = sum([1 for layer in base_model.layers if layer.trainable])\n",
        "\n",
        "# Recompile with lower learning rate\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=CONFIG['phase2_lr']),\n",
        "    loss=loss_fn,\n",
        "    metrics=[\n",
        "        tf.keras.metrics.CategoricalAccuracy(name='accuracy') if CONFIG['use_mixup']\n",
        "        else tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy'),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Phase 2 callbacks\n",
        "phase2_callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=8,\n",
        "        restore_best_weights=True,\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_loss',\n",
        "        factor=0.5,\n",
        "        patience=3,\n",
        "        min_lr=1e-8,\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=f\"{CONFIG['model_save_dir']}/phase2_best.keras\",\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        verbose=1\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 2: Fine-tuning\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Epochs: {CONFIG['phase2_epochs']}\")\n",
        "print(f\"Learning rate: {CONFIG['phase2_lr']}\")\n",
        "print(f\"Base trainable: {base_model.trainable}\")\n",
        "print(f\"Trainable layers in base: {trainable_layers}\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Train phase 2\n",
        "history_phase2 = model.fit(\n",
        "    train_ds_prepared,\n",
        "    validation_data=val_ds_prepared,\n",
        "    epochs=CONFIG['phase2_epochs'],\n",
        "    callbacks=phase2_callbacks,\n",
        "    class_weight=class_weights if not CONFIG['use_mixup'] else None,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\n‚úì Phase 2 training completed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hw7UDz5XqRbT"
      },
      "source": [
        "## 12. Visualize Combined Training History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRfH3QSKqRbT"
      },
      "outputs": [],
      "source": [
        "# Combine histories\n",
        "combined_history = {\n",
        "    'accuracy': history_phase1.history['accuracy'] + history_phase2.history['accuracy'],\n",
        "    'val_accuracy': history_phase1.history['val_accuracy'] + history_phase2.history['val_accuracy'],\n",
        "    'loss': history_phase1.history['loss'] + history_phase2.history['loss'],\n",
        "    'val_loss': history_phase1.history['val_loss'] + history_phase2.history['val_loss']\n",
        "}\n",
        "\n",
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "# Accuracy\n",
        "axes[0].plot(combined_history['accuracy'], label='Train Accuracy', marker='o')\n",
        "axes[0].plot(combined_history['val_accuracy'], label='Val Accuracy', marker='s')\n",
        "axes[0].axvline(x=len(history_phase1.history['accuracy']), color='r', linestyle='--', label='Fine-tuning starts')\n",
        "axes[0].set_title('Model Accuracy (2-Phase Training)', fontsize=14)\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Loss\n",
        "axes[1].plot(combined_history['loss'], label='Train Loss', marker='o')\n",
        "axes[1].plot(combined_history['val_loss'], label='Val Loss', marker='s')\n",
        "axes[1].axvline(x=len(history_phase1.history['loss']), color='r', linestyle='--', label='Fine-tuning starts')\n",
        "axes[1].set_title('Model Loss (2-Phase Training)', fontsize=14)\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final metrics\n",
        "final_train_acc = combined_history['accuracy'][-1]\n",
        "final_val_acc = combined_history['val_accuracy'][-1]\n",
        "best_val_acc = max(combined_history['val_accuracy'])\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL TRAINING METRICS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Final Train Accuracy: {final_train_acc:.4f} ({final_train_acc*100:.2f}%)\")\n",
        "print(f\"Final Val Accuracy:   {final_val_acc:.4f} ({final_val_acc*100:.2f}%)\")\n",
        "print(f\"Best Val Accuracy:    {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
        "print(f\"Overfitting Gap:      {abs(final_train_acc - final_val_acc)*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lwn_Ac2PqRbT"
      },
      "source": [
        "## 13. Load Best Model & Test-Time Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxVECytsqRbT"
      },
      "outputs": [],
      "source": [
        "# Load best model from phase 2\n",
        "best_model = tf.keras.models.load_model(\n",
        "    f\"{CONFIG['model_save_dir']}/phase2_best.keras\",\n",
        "    custom_objects={'FocalLoss': FocalLoss}\n",
        ")\n",
        "print(\"‚úì Best model loaded\")\n",
        "\n",
        "def predict_with_tta(model, image, num_augmentations=5):\n",
        "    \"\"\"\n",
        "    Test-Time Augmentation: make predictions on augmented versions\n",
        "    of the same image and average the results\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    # Original prediction\n",
        "    pred = model.predict(image, verbose=0)\n",
        "    predictions.append(pred)\n",
        "\n",
        "    # Augmented predictions\n",
        "    for _ in range(num_augmentations - 1):\n",
        "        augmented = data_augmentation(image, training=True)\n",
        "        pred = model.predict(augmented, verbose=0)\n",
        "        predictions.append(pred)\n",
        "\n",
        "    # Average predictions\n",
        "    avg_pred = np.mean(predictions, axis=0)\n",
        "\n",
        "    return avg_pred\n",
        "\n",
        "print(\"‚úì Test-Time Augmentation function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iir_k3doqRbU"
      },
      "source": [
        "## 14. Comprehensive Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sZNXvaPqRbU"
      },
      "outputs": [],
      "source": [
        "# Prepare validation dataset without preprocessing for TTA\n",
        "val_ds_raw = tf.keras.utils.image_dataset_from_directory(\n",
        "    CONFIG['data_dir'],\n",
        "    validation_split=CONFIG['validation_split'],\n",
        "    subset=\"validation\",\n",
        "    seed=CONFIG['seed'],\n",
        "    image_size=(CONFIG['img_height'], CONFIG['img_width']),\n",
        "    batch_size=1,  # Process one image at a time for TTA\n",
        "    label_mode='int',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(\"Generating predictions with TTA...\")\n",
        "y_true = []\n",
        "y_pred = []\n",
        "y_pred_probs = []\n",
        "\n",
        "for images, labels in val_ds_raw:\n",
        "    # Preprocess\n",
        "    images_preprocessed = preprocess_input(images)\n",
        "\n",
        "    if CONFIG['use_tta']:\n",
        "        predictions = predict_with_tta(best_model, images_preprocessed, CONFIG['tta_steps'])\n",
        "    else:\n",
        "        predictions = best_model.predict(images_preprocessed, verbose=0)\n",
        "\n",
        "    y_true.append(labels.numpy()[0])\n",
        "    y_pred.append(np.argmax(predictions[0]))\n",
        "    y_pred_probs.append(predictions[0])\n",
        "\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "y_pred_probs = np.array(y_pred_probs)\n",
        "\n",
        "accuracy = np.mean(y_true == y_pred)\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"EVALUATION RESULTS WITH TTA\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Total predictions: {len(y_pred)}\")\n",
        "print(f\"Correct predictions: {np.sum(y_true == y_pred)}\")\n",
        "print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EajTros5qRbU"
      },
      "source": [
        "## 15. Confusion Matrix & Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkac7sUwqRbU"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Blues',\n",
        "    xticklabels=class_names,\n",
        "    yticklabels=class_names,\n",
        "    square=True\n",
        ")\n",
        "plt.title('Confusion Matrix (with TTA)', fontsize=16, pad=20)\n",
        "plt.xlabel('Predicted Label', fontsize=12)\n",
        "plt.ylabel('True Label', fontsize=12)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CLASSIFICATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
        "\n",
        "# Per-class accuracy\n",
        "per_class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
        "\n",
        "print(\"\\nPer-class accuracy:\")\n",
        "for class_name, acc in zip(class_names, per_class_accuracy):\n",
        "    emoji = \"‚úÖ\" if acc >= 0.85 else \"‚ö†Ô∏è\" if acc >= 0.75 else \"‚ùå\"\n",
        "    print(f\"  {emoji} {class_name:12s}: {acc:.4f} ({acc*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnIs4jaSqRbU"
      },
      "source": [
        "## 16. Save Final Model (All Formats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0KzEt1AqRbU"
      },
      "outputs": [],
      "source": [
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "model_name = f\"drgreen_v4_improved_{timestamp}\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"SAVING MODEL: {model_name}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# 1. Keras format\n",
        "keras_path = f\"{CONFIG['model_save_dir']}/{model_name}.keras\"\n",
        "best_model.save(keras_path)\n",
        "print(f\"‚úì Saved Keras format: {keras_path}\")\n",
        "\n",
        "# 2. SavedModel format\n",
        "savedmodel_path = f\"{CONFIG['model_save_dir']}/{model_name}_savedmodel\"\n",
        "tf.saved_model.save(best_model, savedmodel_path)\n",
        "print(f\"‚úì Saved SavedModel: {savedmodel_path}\")\n",
        "\n",
        "# 3. TFLite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(best_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "tflite_path = f\"{CONFIG['model_save_dir']}/{model_name}.tflite\"\n",
        "with open(tflite_path, 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "print(f\"‚úì Saved TFLite: {tflite_path}\")\n",
        "\n",
        "# 4. Save metadata\n",
        "metadata = {\n",
        "    'model_name': model_name,\n",
        "    'created_at': timestamp,\n",
        "    'version': '4.0-improved',\n",
        "    'approach': 'Two-phase training: frozen base + fine-tuning with TTA',\n",
        "    'config': CONFIG,\n",
        "    'class_names': class_names,\n",
        "    'performance': {\n",
        "        'final_accuracy': float(accuracy),\n",
        "        'best_val_accuracy': float(best_val_acc),\n",
        "    },\n",
        "    'per_class_accuracy': {\n",
        "        class_names[i]: float(per_class_accuracy[i])\n",
        "        for i in range(len(class_names))\n",
        "    }\n",
        "}\n",
        "\n",
        "metadata_path = f\"{CONFIG['model_save_dir']}/{model_name}_metadata.json\"\n",
        "with open(metadata_path, 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "print(f\"‚úì Saved metadata: {metadata_path}\")\n",
        "\n",
        "# Save class names\n",
        "class_names_path = f\"{CONFIG['model_save_dir']}/class_names.json\"\n",
        "with open(class_names_path, 'w') as f:\n",
        "    json.dump(class_names, f, indent=2)\n",
        "print(f\"‚úì Saved class names: {class_names_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9ny2mWJqRbV"
      },
      "source": [
        "## 17. Single Image Prediction with TTA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bw6U6gzjqRbV"
      },
      "outputs": [],
      "source": [
        "def predict_single_image(image_path, model, class_names, use_tta=True, show_plot=True):\n",
        "    \"\"\"\n",
        "    Predict plant class for a single image with optional TTA\n",
        "    \"\"\"\n",
        "    # Load and preprocess image\n",
        "    img = tf.keras.utils.load_img(\n",
        "        image_path,\n",
        "        target_size=(CONFIG['img_height'], CONFIG['img_width'])\n",
        "    )\n",
        "    img_array = tf.keras.utils.img_to_array(img)\n",
        "    img_array = preprocess_input(img_array)\n",
        "    img_array = tf.expand_dims(img_array, 0)\n",
        "\n",
        "    # Make prediction\n",
        "    if use_tta:\n",
        "        predictions = predict_with_tta(model, img_array, CONFIG['tta_steps'])\n",
        "    else:\n",
        "        predictions = model.predict(img_array, verbose=0)\n",
        "\n",
        "    predicted_class_idx = np.argmax(predictions[0])\n",
        "    predicted_class = class_names[predicted_class_idx]\n",
        "    confidence = predictions[0][predicted_class_idx]\n",
        "\n",
        "    all_probabilities = {\n",
        "        class_names[i]: float(predictions[0][i])\n",
        "        for i in range(len(class_names))\n",
        "    }\n",
        "\n",
        "    sorted_probs = sorted(all_probabilities.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    if show_plot:\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "        ax1.imshow(img)\n",
        "        title_color = 'green' if confidence > 0.8 else 'orange' if confidence > 0.6 else 'red'\n",
        "        ax1.set_title(\n",
        "            f\"Predicted: {predicted_class}\\nConfidence: {confidence:.2%}\\n{'(with TTA)' if use_tta else ''}\",\n",
        "            fontsize=14,\n",
        "            color=title_color\n",
        "        )\n",
        "        ax1.axis('off')\n",
        "\n",
        "        classes = [x[0] for x in sorted_probs]\n",
        "        probs = [x[1] for x in sorted_probs]\n",
        "        colors = ['green' if i == 0 else 'lightblue' for i in range(len(classes))]\n",
        "\n",
        "        ax2.barh(classes, probs, color=colors)\n",
        "        ax2.set_xlabel('Probability')\n",
        "        ax2.set_title('Class Probabilities')\n",
        "        ax2.set_xlim(0, 1)\n",
        "\n",
        "        for i, (class_name, prob) in enumerate(sorted_probs):\n",
        "            ax2.text(prob + 0.02, i, f'{prob:.2%}', va='center')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return {\n",
        "        'predicted_class': predicted_class,\n",
        "        'confidence': float(confidence),\n",
        "        'all_probabilities': all_probabilities,\n",
        "        'top_3': sorted_probs[:3]\n",
        "    }\n",
        "\n",
        "print(\"‚úì Enhanced prediction function with TTA defined\")\n",
        "print(\"\\nUsage:\")\n",
        "print(\"  result = predict_single_image('leaf.jpg', best_model, class_names, use_tta=True)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSEVyWweqRbV"
      },
      "source": [
        "## 18. Final Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtZf6QlJqRbW"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üåø DR GREEN V4 IMPROVED - TRAINING COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nüìä Model Architecture:\")\n",
        "print(f\"  Base Model: {CONFIG['base_model']}\")\n",
        "print(f\"  Training Strategy: Two-phase (frozen ‚Üí fine-tuning)\")\n",
        "print(f\"  Advanced Techniques: MixUp, TTA, Focal Loss\")\n",
        "\n",
        "print(f\"\\nüéØ Final Performance:\")\n",
        "print(f\"  Validation Accuracy (with TTA): {accuracy*100:.2f}%\")\n",
        "print(f\"  Best Val Accuracy: {best_val_acc*100:.2f}%\")\n",
        "\n",
        "improvement = (accuracy - 0.8206) * 100  # vs v3 baseline\n",
        "print(f\"\\nüìà Improvement over V3:\")\n",
        "print(f\"  +{improvement:.2f}% accuracy gain\")\n",
        "\n",
        "print(f\"\\nüå± Per-Class Performance:\")\n",
        "for class_name, acc in zip(class_names, per_class_accuracy):\n",
        "    emoji = \"‚úÖ\" if acc >= 0.85 else \"‚ö†Ô∏è\" if acc >= 0.75 else \"‚ùå\"\n",
        "    print(f\"  {emoji} {class_name:12s}: {acc*100:5.2f}%\")\n",
        "\n",
        "print(f\"\\nüíæ Models Saved:\")\n",
        "print(f\"  ‚Ä¢ Keras: {model_name}.keras\")\n",
        "print(f\"  ‚Ä¢ TFLite: {model_name}.tflite\")\n",
        "print(f\"  ‚Ä¢ SavedModel: {model_name}_savedmodel/\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üöÄ READY FOR DEPLOYMENT!\")\n",
        "print(\"=\"*60)\n",
        "print(\"Next steps:\")\n",
        "print(\"1. Deploy API with FastAPI\")\n",
        "print(\"2. Build Flutter mobile app\")\n",
        "print(\"3. Deploy to Google Cloud Run\")\n",
        "print(\"4. Set up Firebase integration\")\n",
        "print(\"5. Launch Dr Green app! üéâ\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}