{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/armelyara/drgreen/blob/claude/drgreen-v2-01TfLAqRxjEF2BkLLt72vJrL/drgreen_v8_finetuning_tta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dr Green V8 - Fine-Tuning + Test-Time Augmentation\n",
        "\n",
        "**V7 achieved 70.82% - targeting 85%+ with:**\n",
        "\n",
        "1. **Two-phase training:**\n",
        "   - Phase 1: Frozen base (like V7)\n",
        "   - Phase 2: Fine-tune last 30 layers of MobileNetV2\n",
        "\n",
        "2. **Test-Time Augmentation (TTA):**\n",
        "   - Multiple augmented predictions per image\n",
        "   - Average predictions for more robust results\n",
        "   - Expected improvement: 3-5%\n",
        "\n",
        "3. **Stratified split** (from V7)\n",
        "4. **Focal Loss** (from V6)\n",
        "\n",
        "### Target: >85% accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q gdown\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import json\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import zipfile\n",
        "import os\n",
        "import gdown\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download dataset\n",
        "file_id = '1zI5KfTtuV0BlBQnNDNq4tBJuEkxLZZBD'\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "output = '/content/drgreen.zip'\n",
        "\n",
        "print(\"Downloading dataset...\")\n",
        "gdown.download(url, output, quiet=False)\n",
        "with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content')\n",
        "print(\"Dataset ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CONFIG = {\n",
        "    'data_dir': 'rename',\n",
        "    'model_save_dir': 'models',\n",
        "    'img_height': 224,\n",
        "    'img_width': 224,\n",
        "    'batch_size': 16,\n",
        "    \n",
        "    # Phase 1: Frozen training\n",
        "    'phase1_epochs': 30,\n",
        "    'phase1_lr': 0.001,\n",
        "    \n",
        "    # Phase 2: Fine-tuning\n",
        "    'phase2_epochs': 30,\n",
        "    'phase2_lr': 0.00005,  # Much lower LR for fine-tuning\n",
        "    'fine_tune_at': 100,  # Unfreeze from layer 100 onwards (last ~30 layers)\n",
        "    \n",
        "    'validation_split': 0.2,\n",
        "    'seed': 42,\n",
        "    'num_classes': 4,\n",
        "    \n",
        "    # Model\n",
        "    'dropout_rate': 0.5,\n",
        "    'dense_units': 128,  # Larger head for fine-tuning\n",
        "    'l2_reg': 0.01,\n",
        "    'label_smoothing': 0.1,\n",
        "    \n",
        "    # Focal Loss\n",
        "    'focal_gamma': 2.0,\n",
        "    'focal_alpha': 0.25,\n",
        "    \n",
        "    # TTA\n",
        "    'tta_augmentations': 10,  # Number of augmented predictions per image\n",
        "    \n",
        "    # Callbacks\n",
        "    'early_stopping_patience': 10,\n",
        "    'reduce_lr_patience': 4,\n",
        "}\n",
        "\n",
        "Path(CONFIG['model_save_dir']).mkdir(exist_ok=True)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"DR GREEN V8 - FINE-TUNING + TTA\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nPhase 1: {CONFIG['phase1_epochs']} epochs frozen, LR={CONFIG['phase1_lr']}\")\n",
        "print(f\"Phase 2: {CONFIG['phase2_epochs']} epochs fine-tuning, LR={CONFIG['phase2_lr']}\")\n",
        "print(f\"Fine-tune from layer: {CONFIG['fine_tune_at']}\")\n",
        "print(f\"TTA augmentations: {CONFIG['tta_augmentations']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Focal Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FocalLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, gamma=2.0, alpha=0.25, label_smoothing=0.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.label_smoothing = label_smoothing\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        num_classes = tf.cast(tf.shape(y_true)[-1], tf.float32)\n",
        "        y_true = y_true * (1.0 - self.label_smoothing) + (self.label_smoothing / num_classes)\n",
        "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
        "        cross_entropy = -y_true * tf.math.log(y_pred)\n",
        "        p_t = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
        "        focal_weight = tf.pow(1 - p_t, self.gamma)\n",
        "        return tf.reduce_mean(self.alpha * focal_weight * tf.reduce_sum(cross_entropy, axis=-1))\n",
        "\n",
        "    def get_config(self):\n",
        "        return {'gamma': self.gamma, 'alpha': self.alpha, 'label_smoothing': self.label_smoothing}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Stratified Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all image paths and labels\n",
        "data_dir = Path(CONFIG['data_dir'])\n",
        "class_names = sorted([d.name for d in data_dir.iterdir() if d.is_dir()])\n",
        "\n",
        "all_image_paths = []\n",
        "all_labels = []\n",
        "\n",
        "for class_idx, class_name in enumerate(class_names):\n",
        "    class_dir = data_dir / class_name\n",
        "    for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\n",
        "        for img_path in class_dir.glob(ext):\n",
        "            all_image_paths.append(str(img_path))\n",
        "            all_labels.append(class_idx)\n",
        "\n",
        "all_image_paths = np.array(all_image_paths)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "print(f\"Total images: {len(all_image_paths)}\")\n",
        "for i, name in enumerate(class_names):\n",
        "    print(f\"  {name}: {(all_labels == i).sum()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stratified split\n",
        "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "    all_image_paths, all_labels,\n",
        "    test_size=CONFIG['validation_split'],\n",
        "    random_state=CONFIG['seed'],\n",
        "    stratify=all_labels\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining: {len(train_paths)} images\")\n",
        "print(f\"Validation: {len(val_paths)} images\")\n",
        "\n",
        "# Class weights\n",
        "class_weights = {}\n",
        "for i in range(len(class_names)):\n",
        "    count = (train_labels == i).sum()\n",
        "    class_weights[i] = (len(train_labels) / (len(class_names) * count)) ** 1.2\n",
        "\n",
        "print(\"\\nClass weights:\")\n",
        "for i, w in class_weights.items():\n",
        "    print(f\"  {class_names[i]}: {w:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create datasets\n",
        "def load_image(path, label):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, [CONFIG['img_height'], CONFIG['img_width']])\n",
        "    return img, label\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
        "train_ds = train_ds.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.map(lambda x, y: (x, tf.one_hot(y, CONFIG['num_classes'])))\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
        "val_ds = val_ds.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.map(lambda x, y: (x, tf.one_hot(y, CONFIG['num_classes'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training augmentation\n",
        "train_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    tf.keras.layers.RandomRotation(0.3),\n",
        "    tf.keras.layers.RandomZoom(0.2),\n",
        "    tf.keras.layers.RandomBrightness(0.2),\n",
        "    tf.keras.layers.RandomContrast(0.2),\n",
        "    tf.keras.layers.RandomTranslation(0.1, 0.1),\n",
        "], name=\"train_augmentation\")\n",
        "\n",
        "# TTA augmentation (lighter for inference)\n",
        "tta_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
        "    tf.keras.layers.RandomRotation(0.15),\n",
        "    tf.keras.layers.RandomZoom(0.1),\n",
        "    tf.keras.layers.RandomBrightness(0.1),\n",
        "], name=\"tta_augmentation\")\n",
        "\n",
        "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "\n",
        "# Apply to training\n",
        "train_ds = train_ds.map(lambda x, y: (train_augmentation(x, training=True), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.map(lambda x, y: (preprocess_input(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "train_ds = train_ds.shuffle(1000).batch(CONFIG['batch_size']).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Validation (no augmentation)\n",
        "val_ds = val_ds.map(lambda x, y: (preprocess_input(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.batch(CONFIG['batch_size']).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(\"Data pipelines ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_model():\n",
        "    inputs = tf.keras.Input(shape=(CONFIG['img_height'], CONFIG['img_width'], 3))\n",
        "    \n",
        "    base_model = tf.keras.applications.MobileNetV2(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_tensor=inputs,\n",
        "        pooling='avg'\n",
        "    )\n",
        "    base_model.trainable = False  # Start frozen\n",
        "    \n",
        "    x = base_model.output\n",
        "    x = tf.keras.layers.Dropout(CONFIG['dropout_rate'])(x)\n",
        "    x = tf.keras.layers.Dense(\n",
        "        CONFIG['dense_units'],\n",
        "        activation='relu',\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(CONFIG['l2_reg'])\n",
        "    )(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(CONFIG['dropout_rate'] * 0.5)(x)\n",
        "    outputs = tf.keras.layers.Dense(\n",
        "        CONFIG['num_classes'],\n",
        "        activation='softmax',\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(CONFIG['l2_reg'])\n",
        "    )(x)\n",
        "    \n",
        "    return tf.keras.Model(inputs, outputs, name='DrGreen_V8'), base_model\n",
        "\n",
        "model, base_model = build_model()\n",
        "\n",
        "print(f\"\\nModel built: {model.count_params():,} parameters\")\n",
        "print(f\"MobileNetV2 has {len(base_model.layers)} layers\")\n",
        "print(f\"Will fine-tune from layer {CONFIG['fine_tune_at']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Phase 1: Frozen Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 1: FROZEN BASE TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=CONFIG['phase1_lr']),\n",
        "    loss=FocalLoss(\n",
        "        gamma=CONFIG['focal_gamma'],\n",
        "        alpha=CONFIG['focal_alpha'],\n",
        "        label_smoothing=CONFIG['label_smoothing']\n",
        "    ),\n",
        "    metrics=[\n",
        "        tf.keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "        tf.keras.metrics.TopKCategoricalAccuracy(k=2, name='top2_accuracy')\n",
        "    ]\n",
        ")\n",
        "\n",
        "callbacks_phase1 = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=CONFIG['early_stopping_patience'],\n",
        "        restore_best_weights=True,\n",
        "        mode='max'\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_accuracy',\n",
        "        factor=0.5,\n",
        "        patience=CONFIG['reduce_lr_patience'],\n",
        "        min_lr=1e-6,\n",
        "        mode='max'\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=f\"{CONFIG['model_save_dir']}/best_model_v8_phase1.keras\",\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        mode='max'\n",
        "    )\n",
        "]\n",
        "\n",
        "history_phase1 = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=CONFIG['phase1_epochs'],\n",
        "    callbacks=callbacks_phase1,\n",
        "    class_weight=class_weights,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "phase1_best_acc = max(history_phase1.history['val_accuracy'])\n",
        "print(f\"\\nPhase 1 complete! Best val accuracy: {phase1_best_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Phase 2: Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PHASE 2: FINE-TUNING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Unfreeze layers from fine_tune_at onwards\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:CONFIG['fine_tune_at']]:\n",
        "    layer.trainable = False\n",
        "\n",
        "trainable_params = sum([tf.size(v).numpy() for v in model.trainable_variables])\n",
        "print(f\"\\nTrainable parameters after unfreezing: {trainable_params:,}\")\n",
        "print(f\"Fine-tuning {len(base_model.layers) - CONFIG['fine_tune_at']} layers\")\n",
        "\n",
        "# Recompile with lower learning rate\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=CONFIG['phase2_lr']),\n",
        "    loss=FocalLoss(\n",
        "        gamma=CONFIG['focal_gamma'],\n",
        "        alpha=CONFIG['focal_alpha'],\n",
        "        label_smoothing=CONFIG['label_smoothing']\n",
        "    ),\n",
        "    metrics=[\n",
        "        tf.keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "        tf.keras.metrics.TopKCategoricalAccuracy(k=2, name='top2_accuracy')\n",
        "    ]\n",
        ")\n",
        "\n",
        "callbacks_phase2 = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=CONFIG['early_stopping_patience'],\n",
        "        restore_best_weights=True,\n",
        "        mode='max'\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_accuracy',\n",
        "        factor=0.5,\n",
        "        patience=CONFIG['reduce_lr_patience'],\n",
        "        min_lr=1e-7,\n",
        "        mode='max'\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=f\"{CONFIG['model_save_dir']}/best_model_v8.keras\",\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        mode='max'\n",
        "    )\n",
        "]\n",
        "\n",
        "history_phase2 = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=CONFIG['phase2_epochs'],\n",
        "    callbacks=callbacks_phase2,\n",
        "    class_weight=class_weights,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "phase2_best_acc = max(history_phase2.history['val_accuracy'])\n",
        "print(f\"\\nPhase 2 complete! Best val accuracy: {phase2_best_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Training Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine histories\n",
        "total_epochs = len(history_phase1.history['accuracy']) + len(history_phase2.history['accuracy'])\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy\n",
        "p1_acc = history_phase1.history['accuracy']\n",
        "p1_val_acc = history_phase1.history['val_accuracy']\n",
        "p2_acc = history_phase2.history['accuracy']\n",
        "p2_val_acc = history_phase2.history['val_accuracy']\n",
        "\n",
        "axes[0].plot(range(1, len(p1_acc)+1), p1_acc, 'b-', label='Train P1')\n",
        "axes[0].plot(range(1, len(p1_val_acc)+1), p1_val_acc, 'b--', label='Val P1')\n",
        "axes[0].plot(range(len(p1_acc)+1, len(p1_acc)+len(p2_acc)+1), p2_acc, 'r-', label='Train P2')\n",
        "axes[0].plot(range(len(p1_val_acc)+1, len(p1_val_acc)+len(p2_val_acc)+1), p2_val_acc, 'r--', label='Val P2')\n",
        "axes[0].axvline(x=len(p1_acc), color='g', linestyle=':', label='Fine-tune start')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_title('Training Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Loss\n",
        "p1_loss = history_phase1.history['loss']\n",
        "p1_val_loss = history_phase1.history['val_loss']\n",
        "p2_loss = history_phase2.history['loss']\n",
        "p2_val_loss = history_phase2.history['val_loss']\n",
        "\n",
        "axes[1].plot(range(1, len(p1_loss)+1), p1_loss, 'b-', label='Train P1')\n",
        "axes[1].plot(range(1, len(p1_val_loss)+1), p1_val_loss, 'b--', label='Val P1')\n",
        "axes[1].plot(range(len(p1_loss)+1, len(p1_loss)+len(p2_loss)+1), p2_loss, 'r-', label='Train P2')\n",
        "axes[1].plot(range(len(p1_val_loss)+1, len(p1_val_loss)+len(p2_val_loss)+1), p2_val_loss, 'r--', label='Val P2')\n",
        "axes[1].axvline(x=len(p1_loss), color='g', linestyle=':', label='Fine-tune start')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].set_title('Training Loss')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nPhase 1 best: {phase1_best_acc*100:.2f}%\")\n",
        "print(f\"Phase 2 best: {phase2_best_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Test-Time Augmentation (TTA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model\n",
        "best_model = tf.keras.models.load_model(\n",
        "    f\"{CONFIG['model_save_dir']}/best_model_v8.keras\",\n",
        "    custom_objects={'FocalLoss': FocalLoss}\n",
        ")\n",
        "print(\"Best model loaded\")\n",
        "\n",
        "def predict_with_tta(model, image_paths, labels, n_augmentations=10):\n",
        "    \"\"\"\n",
        "    Test-Time Augmentation: Average predictions from multiple augmented versions\n",
        "    \"\"\"\n",
        "    all_predictions = []\n",
        "    \n",
        "    for img_path in image_paths:\n",
        "        # Load image\n",
        "        img = tf.io.read_file(img_path)\n",
        "        img = tf.image.decode_jpeg(img, channels=3)\n",
        "        img = tf.image.resize(img, [CONFIG['img_height'], CONFIG['img_width']])\n",
        "        \n",
        "        # Collect predictions from original + augmented versions\n",
        "        predictions = []\n",
        "        \n",
        "        # Original image\n",
        "        img_preprocessed = preprocess_input(tf.expand_dims(img, 0))\n",
        "        pred = model.predict(img_preprocessed, verbose=0)\n",
        "        predictions.append(pred[0])\n",
        "        \n",
        "        # Augmented versions\n",
        "        for _ in range(n_augmentations - 1):\n",
        "            img_aug = tta_augmentation(img, training=True)\n",
        "            img_preprocessed = preprocess_input(tf.expand_dims(img_aug, 0))\n",
        "            pred = model.predict(img_preprocessed, verbose=0)\n",
        "            predictions.append(pred[0])\n",
        "        \n",
        "        # Average predictions\n",
        "        avg_pred = np.mean(predictions, axis=0)\n",
        "        all_predictions.append(avg_pred)\n",
        "    \n",
        "    return np.array(all_predictions)\n",
        "\n",
        "print(f\"\\nRunning TTA with {CONFIG['tta_augmentations']} augmentations per image...\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "tta_predictions = predict_with_tta(\n",
        "    best_model,\n",
        "    val_paths,\n",
        "    val_labels,\n",
        "    n_augmentations=CONFIG['tta_augmentations']\n",
        ")\n",
        "\n",
        "y_pred_tta = np.argmax(tta_predictions, axis=1)\n",
        "y_true = val_labels\n",
        "\n",
        "tta_accuracy = np.mean(y_pred_tta == y_true)\n",
        "print(f\"\\nTTA Accuracy: {tta_accuracy*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare with standard inference\n",
        "print(\"\\nComparing TTA vs Standard inference...\")\n",
        "\n",
        "# Standard predictions\n",
        "val_ds_eval = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
        "val_ds_eval = val_ds_eval.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds_eval = val_ds_eval.map(lambda x, y: (preprocess_input(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "val_ds_eval = val_ds_eval.batch(CONFIG['batch_size'])\n",
        "\n",
        "y_pred_standard = []\n",
        "for images, labels in val_ds_eval:\n",
        "    preds = best_model.predict(images, verbose=0)\n",
        "    y_pred_standard.extend(np.argmax(preds, axis=1))\n",
        "y_pred_standard = np.array(y_pred_standard)\n",
        "\n",
        "standard_accuracy = np.mean(y_pred_standard == y_true)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TTA IMPROVEMENT\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Standard accuracy: {standard_accuracy*100:.2f}%\")\n",
        "print(f\"TTA accuracy:      {tta_accuracy*100:.2f}%\")\n",
        "print(f\"Improvement:       {(tta_accuracy - standard_accuracy)*100:+.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Detailed Evaluation with TTA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prediction distribution\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREDICTION DISTRIBUTION (TTA)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "pred_counts = {name: 0 for name in class_names}\n",
        "for p in y_pred_tta:\n",
        "    pred_counts[class_names[p]] += 1\n",
        "\n",
        "collapse = False\n",
        "for name, count in pred_counts.items():\n",
        "    pct = count/len(y_pred_tta)*100\n",
        "    if pct > 50:\n",
        "        collapse = True\n",
        "    print(f\"  {name}: {count} ({pct:.1f}%)\")\n",
        "\n",
        "if not collapse:\n",
        "    print(\"\\nPredictions are balanced!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrix with TTA\n",
        "cm = confusion_matrix(y_true, y_pred_tta)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Blues',\n",
        "    xticklabels=class_names,\n",
        "    yticklabels=class_names,\n",
        "    square=True\n",
        ")\n",
        "plt.title(f'Confusion Matrix - V8 with TTA ({tta_accuracy*100:.1f}%)', fontsize=14)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CLASSIFICATION REPORT (TTA)\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(y_true, y_pred_tta, target_names=class_names, digits=4))\n",
        "\n",
        "# Per-class accuracy\n",
        "print(\"\\nPer-class accuracy:\")\n",
        "for i, class_name in enumerate(class_names):\n",
        "    class_mask = y_true == i\n",
        "    if class_mask.sum() > 0:\n",
        "        class_acc = (y_pred_tta[class_mask] == i).mean()\n",
        "        status = \"GOOD\" if class_acc >= 0.75 else \"OK\" if class_acc >= 0.60 else \"LOW\"\n",
        "        print(f\"  [{status}] {class_name}: {class_acc*100:.2f}% ({class_mask.sum()} samples)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "model_name = f\"drgreen_v8_finetuned_tta_{timestamp}\"\n",
        "\n",
        "print(f\"Saving model: {model_name}\")\n",
        "\n",
        "# Keras\n",
        "keras_path = f\"{CONFIG['model_save_dir']}/{model_name}.keras\"\n",
        "best_model.save(keras_path)\n",
        "\n",
        "# TFLite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(best_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "tflite_path = f\"{CONFIG['model_save_dir']}/{model_name}.tflite\"\n",
        "with open(tflite_path, 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "# Metadata\n",
        "metadata = {\n",
        "    'model_name': model_name,\n",
        "    'version': '8.0-finetuned-tta',\n",
        "    'created_at': timestamp,\n",
        "    'class_names': class_names,\n",
        "    'performance': {\n",
        "        'phase1_best_accuracy': float(phase1_best_acc),\n",
        "        'phase2_best_accuracy': float(phase2_best_acc),\n",
        "        'standard_accuracy': float(standard_accuracy),\n",
        "        'tta_accuracy': float(tta_accuracy),\n",
        "        'tta_augmentations': CONFIG['tta_augmentations']\n",
        "    },\n",
        "    'config': {\n",
        "        'fine_tune_at': CONFIG['fine_tune_at'],\n",
        "        'phase1_epochs': CONFIG['phase1_epochs'],\n",
        "        'phase2_epochs': CONFIG['phase2_epochs']\n",
        "    }\n",
        "}\n",
        "\n",
        "metadata_path = f\"{CONFIG['model_save_dir']}/{model_name}_metadata.json\"\n",
        "with open(metadata_path, 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "\n",
        "# Sizes\n",
        "keras_size = os.path.getsize(keras_path) / (1024*1024)\n",
        "tflite_size = os.path.getsize(tflite_path) / (1024*1024)\n",
        "print(f\"  Keras: {keras_size:.2f} MB\")\n",
        "print(f\"  TFLite: {tflite_size:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Final Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DR GREEN V8 - TRAINING COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nTwo-Phase Training:\")\n",
        "print(f\"  Phase 1 (frozen): {phase1_best_acc*100:.2f}%\")\n",
        "print(f\"  Phase 2 (fine-tuned): {phase2_best_acc*100:.2f}%\")\n",
        "\n",
        "print(f\"\\nTest-Time Augmentation:\")\n",
        "print(f\"  Standard: {standard_accuracy*100:.2f}%\")\n",
        "print(f\"  With TTA: {tta_accuracy*100:.2f}%\")\n",
        "print(f\"  Improvement: {(tta_accuracy - standard_accuracy)*100:+.2f}%\")\n",
        "\n",
        "print(f\"\\nPrediction Distribution:\")\n",
        "for name, count in pred_counts.items():\n",
        "    print(f\"  {name}: {count/len(y_pred_tta)*100:.1f}%\")\n",
        "\n",
        "print(f\"\\nModel saved: {model_name}\")\n",
        "\n",
        "if tta_accuracy >= 0.85:\n",
        "    print(\"\\nTARGET ACHIEVED! Ready for deployment.\")\n",
        "elif tta_accuracy >= 0.80:\n",
        "    print(\"\\nGood performance! Consider more data for 85%+.\")\n",
        "else:\n",
        "    print(\"\\nMore optimization needed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 14. Download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(keras_path)\n",
        "files.download(tflite_path)\n",
        "files.download(metadata_path)\n",
        "print(\"Downloads initiated!\")"
      ]
    }
  ]
}
