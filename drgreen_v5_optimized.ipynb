{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/armelyara/drgreen/blob/claude/drgreen-v2-01TfLAqRxjEF2BkLLt72vJrL/drgreen_v5_optimized.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåø Dr Green V5 - Optimized for Small Datasets\n",
    "\n",
    "**Best approach for ~1000 images: Simple architecture, strong regularization**\n",
    "\n",
    "### Why V5 is different:\n",
    "- **Lighter model**: EfficientNetB0 (4M params vs 12M for B3)\n",
    "- **No fine-tuning**: Frozen base only (prevents overfitting)\n",
    "- **Strong augmentation**: Aggressive data augmentation\n",
    "- **Label smoothing**: Better generalization\n",
    "- **Cosine decay**: Optimal learning rate schedule\n",
    "\n",
    "### Target: >85% accuracy with <5% overfitting gap\n",
    "\n",
    "### Plant Classes:\n",
    "1. Artemisia (Armoise) - Antimalarial\n",
    "2. Carica (Papaya) - Digestive aid\n",
    "3. Goyavier (Guava) - Antiseptic\n",
    "4. Kinkeliba - Detoxifying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install gdown for dataset download\n",
    "!pip install -q gdown\n",
    "\n",
    "# Core imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import zipfile\n",
    "import os\n",
    "import gdown\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "\n",
    "# Check GPU\n",
    "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "    print(\"‚úÖ GPU detected - training will be fast!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU - training will be slow. Enable GPU in Runtime > Change runtime type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from Google Drive\n",
    "file_id = '1zI5KfTtuV0BlBQnNDNq4tBJuEkxLZZBD'\n",
    "url = f'https://drive.google.com/uc?id={file_id}'\n",
    "output = '/content/drgreen.zip'\n",
    "\n",
    "print(\"üì• Downloading dataset from Google Drive...\")\n",
    "try:\n",
    "    gdown.download(url, output, quiet=False)\n",
    "    print(\"‚úì Dataset downloaded!\")\n",
    "    \n",
    "    # Extract\n",
    "    print(\"\\nüìÇ Extracting...\")\n",
    "    with zipfile.ZipFile(output, 'r') as zip_ref:\n",
    "        zip_ref.extractall('/content')\n",
    "    print(\"‚úì Dataset extracted!\")\n",
    "except:\n",
    "    print(\"\\n‚ö†Ô∏è Auto-download failed. Please upload manually:\")\n",
    "    print(\"from google.colab import files\")\n",
    "    print(\"uploaded = files.upload()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration (Optimized for Small Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V5 Configuration - Optimized for ~1000 images\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    'data_dir': 'rename',\n",
    "    'model_save_dir': 'models',\n",
    "    \n",
    "    # Image parameters\n",
    "    'img_height': 224,  # Standard size for EfficientNetB0\n",
    "    'img_width': 224,\n",
    "    'batch_size': 32,\n",
    "    \n",
    "    # Training parameters - SINGLE PHASE (no fine-tuning)\n",
    "    'epochs': 50,\n",
    "    'initial_lr': 0.001,\n",
    "    \n",
    "    'validation_split': 0.2,\n",
    "    'seed': 42,\n",
    "    \n",
    "    # Model parameters\n",
    "    'base_model': 'EfficientNetB0',  # Lighter than B3\n",
    "    'dropout_rate': 0.5,\n",
    "    'num_classes': 4,\n",
    "    'dense_units': 128,  # Smaller than V4\n",
    "    \n",
    "    # Regularization\n",
    "    'l2_reg': 0.01,\n",
    "    'label_smoothing': 0.1,  # Helps generalization\n",
    "    \n",
    "    # Callbacks\n",
    "    'early_stopping_patience': 10,\n",
    "    'reduce_lr_patience': 5,\n",
    "    'reduce_lr_factor': 0.5,\n",
    "    'min_lr': 1e-7,\n",
    "}\n",
    "\n",
    "PLANT_CLASSES = ['artemisia', 'carica', 'goyavier', 'kinkeliba']\n",
    "Path(CONFIG['model_save_dir']).mkdir(exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üåø DR GREEN V5 - OPTIMIZED FOR SMALL DATASETS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBase Model: {CONFIG['base_model']} (FROZEN - no fine-tuning)\")\n",
    "print(f\"Image Size: {CONFIG['img_height']}x{CONFIG['img_width']}\")\n",
    "print(f\"Batch Size: {CONFIG['batch_size']}\")\n",
    "print(f\"Epochs: {CONFIG['epochs']}\")\n",
    "print(f\"Learning Rate: {CONFIG['initial_lr']}\")\n",
    "print(f\"\\nRegularization:\")\n",
    "print(f\"  - Dropout: {CONFIG['dropout_rate']}\")\n",
    "print(f\"  - L2: {CONFIG['l2_reg']}\")\n",
    "print(f\"  - Label Smoothing: {CONFIG['label_smoothing']}\")\n",
    "print(\"\\n‚úÖ Single-phase training (frozen base)\")\n",
    "print(\"‚úÖ Strong data augmentation\")\n",
    "print(\"‚úÖ Cosine decay learning rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    CONFIG['data_dir'],\n",
    "    validation_split=CONFIG['validation_split'],\n",
    "    subset=\"training\",\n",
    "    seed=CONFIG['seed'],\n",
    "    image_size=(CONFIG['img_height'], CONFIG['img_width']),\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    label_mode='int'\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    CONFIG['data_dir'],\n",
    "    validation_split=CONFIG['validation_split'],\n",
    "    subset=\"validation\",\n",
    "    seed=CONFIG['seed'],\n",
    "    image_size=(CONFIG['img_height'], CONFIG['img_width']),\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    label_mode='int'\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(f\"\\n‚úÖ Classes found: {class_names}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count samples per class\n",
    "class_counts = {name: 0 for name in class_names}\n",
    "\n",
    "for images, labels in train_ds:\n",
    "    for label in labels.numpy():\n",
    "        class_counts[class_names[label]] += 1\n",
    "\n",
    "val_class_counts = {name: 0 for name in class_names}\n",
    "for images, labels in val_ds:\n",
    "    for label in labels.numpy():\n",
    "        val_class_counts[class_names[label]] += 1\n",
    "\n",
    "print(\"\\nüìä Dataset Statistics:\")\n",
    "print(\"\\nTraining set:\")\n",
    "total_train = sum(class_counts.values())\n",
    "for class_name, count in class_counts.items():\n",
    "    print(f\"  {class_name}: {count} images ({count/total_train*100:.1f}%)\")\n",
    "print(f\"  Total: {total_train}\")\n",
    "\n",
    "print(\"\\nValidation set:\")\n",
    "total_val = sum(val_class_counts.values())\n",
    "for class_name, count in val_class_counts.items():\n",
    "    print(f\"  {class_name}: {count} images ({count/total_val*100:.1f}%)\")\n",
    "print(f\"  Total: {total_val}\")\n",
    "\n",
    "# Calculate class weights\n",
    "total_samples = sum(class_counts.values())\n",
    "class_weights = {}\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_weights[i] = total_samples / (len(class_names) * class_counts[class_name])\n",
    "\n",
    "print(\"\\n‚öñÔ∏è Class weights (for imbalance):\")\n",
    "for i, weight in class_weights.items():\n",
    "    print(f\"  {class_names[i]}: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Strong Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strong data augmentation for small dataset\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    tf.keras.layers.RandomRotation(0.3),\n",
    "    tf.keras.layers.RandomZoom(0.2),\n",
    "    tf.keras.layers.RandomBrightness(0.2),\n",
    "    tf.keras.layers.RandomContrast(0.2),\n",
    "    tf.keras.layers.RandomTranslation(0.1, 0.1),\n",
    "], name=\"strong_augmentation\")\n",
    "\n",
    "# Preprocessing for EfficientNet\n",
    "preprocess_input = tf.keras.applications.efficientnet.preprocess_input\n",
    "\n",
    "print(\"‚úÖ Strong data augmentation configured:\")\n",
    "for layer in data_augmentation.layers:\n",
    "    print(f\"  - {layer.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize augmentation\n",
    "plt.figure(figsize=(12, 8))\n",
    "for images, labels in train_ds.take(1):\n",
    "    image = images[0]\n",
    "    plt.subplot(3, 4, 1)\n",
    "    plt.imshow(image.numpy().astype(\"uint8\"))\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "    for i in range(11):\n",
    "        augmented = data_augmentation(tf.expand_dims(image, 0), training=True)\n",
    "        plt.subplot(3, 4, i + 2)\n",
    "        plt.imshow(augmented[0].numpy().astype(\"uint8\"))\n",
    "        plt.title(f\"Aug {i+1}\")\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Data Augmentation Examples\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Training: augmentation + preprocessing\n",
    "train_ds = train_ds.map(\n",
    "    lambda x, y: (data_augmentation(x, training=True), y),\n",
    "    num_parallel_calls=AUTOTUNE\n",
    ")\n",
    "train_ds = train_ds.map(\n",
    "    lambda x, y: (preprocess_input(x), y),\n",
    "    num_parallel_calls=AUTOTUNE\n",
    ")\n",
    "\n",
    "# Validation: only preprocessing\n",
    "val_ds = val_ds.map(\n",
    "    lambda x, y: (preprocess_input(x), y),\n",
    "    num_parallel_calls=AUTOTUNE\n",
    ")\n",
    "\n",
    "# Optimize pipeline\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(AUTOTUNE)\n",
    "\n",
    "print(\"‚úÖ Data pipeline optimized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build Model (Frozen Base - Optimal for Small Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"\n",
    "    Build EfficientNetB0 with FROZEN base\n",
    "    Optimal for small datasets - prevents overfitting\n",
    "    \"\"\"\n",
    "    # Input\n",
    "    inputs = tf.keras.Input(shape=(CONFIG['img_height'], CONFIG['img_width'], 3))\n",
    "    \n",
    "    # Base model - FROZEN\n",
    "    base_model = tf.keras.applications.EfficientNetB0(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=inputs,\n",
    "        pooling='avg'\n",
    "    )\n",
    "    base_model.trainable = False  # IMPORTANT: Keep frozen!\n",
    "    \n",
    "    # Classification head with strong regularization\n",
    "    x = base_model.output\n",
    "    \n",
    "    # Dropout\n",
    "    x = tf.keras.layers.Dropout(CONFIG['dropout_rate'])(x)\n",
    "    \n",
    "    # Dense layer with L2 regularization\n",
    "    x = tf.keras.layers.Dense(\n",
    "        CONFIG['dense_units'],\n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(CONFIG['l2_reg'])\n",
    "    )(x)\n",
    "    \n",
    "    # Batch normalization\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    \n",
    "    # Another dropout\n",
    "    x = tf.keras.layers.Dropout(CONFIG['dropout_rate'] / 2)(x)\n",
    "    \n",
    "    # Output\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "        CONFIG['num_classes'],\n",
    "        activation='softmax',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(CONFIG['l2_reg'])\n",
    "    )(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, outputs, name='DrGreen_V5_Optimized')\n",
    "    \n",
    "    return model, base_model\n",
    "\n",
    "# Build model\n",
    "model, base_model = build_model()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Base: {CONFIG['base_model']} (FROZEN)\")\n",
    "print(f\"Base trainable: {base_model.trainable}\")\n",
    "print(f\"Total parameters: {model.count_params():,}\")\n",
    "trainable = sum([tf.size(v).numpy() for v in model.trainable_variables])\n",
    "print(f\"Trainable parameters: {trainable:,}\")\n",
    "print(f\"Non-trainable: {model.count_params() - trainable:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compile Model with Cosine Decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate steps\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "total_steps = steps_per_epoch * CONFIG['epochs']\n",
    "\n",
    "# Cosine decay learning rate\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=CONFIG['initial_lr'],\n",
    "    decay_steps=total_steps,\n",
    "    alpha=0.01  # End at 1% of initial LR\n",
    ")\n",
    "\n",
    "# Compile with label smoothing\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(label_smoothing=CONFIG['label_smoothing']),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy'),\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=2, name='top2_accuracy')\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPILED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Optimizer: Adam with Cosine Decay\")\n",
    "print(f\"Initial LR: {CONFIG['initial_lr']}\")\n",
    "print(f\"Loss: SparseCategoricalCrossentropy\")\n",
    "print(f\"Label Smoothing: {CONFIG['label_smoothing']}\")\n",
    "print(f\"Total steps: {total_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Setup Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    # Early stopping\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=CONFIG['early_stopping_patience'],\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reduce LR on plateau (backup to cosine decay)\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=CONFIG['reduce_lr_factor'],\n",
    "        patience=CONFIG['reduce_lr_patience'],\n",
    "        min_lr=CONFIG['min_lr'],\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Save best model\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=f\"{CONFIG['model_save_dir']}/best_model_v5.keras\",\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # CSV logger\n",
    "    tf.keras.callbacks.CSVLogger(\n",
    "        f\"{CONFIG['model_save_dir']}/training_log_v5.csv\"\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"‚úÖ Callbacks configured:\")\n",
    "for cb in callbacks:\n",
    "    print(f\"  - {cb.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Epochs: {CONFIG['epochs']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Training samples: ~{total_train}\")\n",
    "print(f\"Validation samples: ~{total_val}\")\n",
    "print(f\"Class weights: Enabled\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Train!\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=CONFIG['epochs'],\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TRAINING COMPLETED\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(history.history['accuracy'], label='Train', marker='o')\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation', marker='s')\n",
    "axes[0].set_title('Model Accuracy', fontsize=14)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(history.history['loss'], label='Train', marker='o')\n",
    "axes[1].plot(history.history['val_loss'], label='Validation', marker='s')\n",
    "axes[1].set_title('Model Loss', fontsize=14)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Top-2 Accuracy\n",
    "axes[2].plot(history.history['top2_accuracy'], label='Train', marker='o')\n",
    "axes[2].plot(history.history['val_top2_accuracy'], label='Validation', marker='s')\n",
    "axes[2].set_title('Top-2 Accuracy', fontsize=14)\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Top-2 Accuracy')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print metrics\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "best_val_acc = max(history.history['val_accuracy'])\n",
    "best_epoch = history.history['val_accuracy'].index(best_val_acc) + 1\n",
    "overfitting_gap = abs(final_train_acc - final_val_acc)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL TRAINING METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Final Train Accuracy: {final_train_acc:.4f} ({final_train_acc*100:.2f}%)\")\n",
    "print(f\"Final Val Accuracy:   {final_val_acc:.4f} ({final_val_acc*100:.2f}%)\")\n",
    "print(f\"Best Val Accuracy:    {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
    "print(f\"Best Epoch:           {best_epoch}\")\n",
    "print(f\"Overfitting Gap:      {overfitting_gap:.4f} ({overfitting_gap*100:.2f}%)\")\n",
    "\n",
    "if overfitting_gap < 0.05:\n",
    "    print(\"\\n‚úÖ Excellent generalization!\")\n",
    "elif overfitting_gap < 0.10:\n",
    "    print(\"\\n‚úÖ Good generalization\")\n",
    "elif overfitting_gap < 0.15:\n",
    "    print(\"\\n‚ö†Ô∏è Moderate overfitting\")\n",
    "else:\n",
    "    print(\"\\n‚ùå High overfitting - consider more regularization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Load Best Model & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_model = tf.keras.models.load_model(f\"{CONFIG['model_save_dir']}/best_model_v5.keras\")\n",
    "print(\"‚úÖ Best model loaded\")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nEvaluating on validation set...\")\n",
    "results = best_model.evaluate(val_ds, verbose=1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Validation Loss:      {results[0]:.4f}\")\n",
    "print(f\"Validation Accuracy:  {results[1]:.4f} ({results[1]*100:.2f}%)\")\n",
    "print(f\"Validation Top-2 Acc: {results[2]:.4f} ({results[2]*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Detailed Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "print(\"Generating predictions...\")\n",
    "\n",
    "# Reload validation set for predictions\n",
    "val_ds_eval = tf.keras.utils.image_dataset_from_directory(\n",
    "    CONFIG['data_dir'],\n",
    "    validation_split=CONFIG['validation_split'],\n",
    "    subset=\"validation\",\n",
    "    seed=CONFIG['seed'],\n",
    "    image_size=(CONFIG['img_height'], CONFIG['img_width']),\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    label_mode='int',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Preprocess\n",
    "val_ds_eval = val_ds_eval.map(\n",
    "    lambda x, y: (preprocess_input(x), y),\n",
    "    num_parallel_calls=AUTOTUNE\n",
    ")\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for images, labels in val_ds_eval:\n",
    "    predictions = best_model.predict(images, verbose=0)\n",
    "    y_true.extend(labels.numpy())\n",
    "    y_pred.extend(np.argmax(predictions, axis=1))\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "accuracy = np.mean(y_true == y_pred)\n",
    "print(f\"\\n‚úÖ Predictions generated\")\n",
    "print(f\"Total: {len(y_pred)}\")\n",
    "print(f\"Correct: {np.sum(y_true == y_pred)}\")\n",
    "print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    square=True,\n",
    "    linewidths=0.5\n",
    ")\n",
    "plt.title('Confusion Matrix - Dr Green V5', fontsize=16, pad=20)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('True', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "# Per-class accuracy\n",
    "per_class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "\n",
    "print(\"\\nPer-class accuracy:\")\n",
    "for class_name, acc in zip(class_names, per_class_accuracy):\n",
    "    emoji = \"‚úÖ\" if acc >= 0.80 else \"‚ö†Ô∏è\" if acc >= 0.70 else \"‚ùå\"\n",
    "    print(f\"  {emoji} {class_name:12s}: {acc:.4f} ({acc*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_name = f\"drgreen_v5_optimized_{timestamp}\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"SAVING MODEL: {model_name}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Keras format\n",
    "keras_path = f\"{CONFIG['model_save_dir']}/{model_name}.keras\"\n",
    "best_model.save(keras_path)\n",
    "print(f\"‚úì Keras: {keras_path}\")\n",
    "\n",
    "# 2. TFLite (optimized for mobile)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(best_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "tflite_path = f\"{CONFIG['model_save_dir']}/{model_name}.tflite\"\n",
    "with open(tflite_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "print(f\"‚úì TFLite: {tflite_path}\")\n",
    "\n",
    "# 3. Metadata\n",
    "metadata = {\n",
    "    'model_name': model_name,\n",
    "    'version': '5.0-optimized',\n",
    "    'created_at': timestamp,\n",
    "    'approach': 'Frozen EfficientNetB0 with strong regularization',\n",
    "    'config': CONFIG,\n",
    "    'class_names': class_names,\n",
    "    'performance': {\n",
    "        'val_accuracy': float(results[1]),\n",
    "        'val_top2_accuracy': float(results[2]),\n",
    "        'val_loss': float(results[0]),\n",
    "        'best_val_accuracy': float(best_val_acc),\n",
    "        'overfitting_gap': float(overfitting_gap)\n",
    "    },\n",
    "    'per_class_accuracy': {\n",
    "        class_names[i]: float(per_class_accuracy[i])\n",
    "        for i in range(len(class_names))\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = f\"{CONFIG['model_save_dir']}/{model_name}_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"‚úì Metadata: {metadata_path}\")\n",
    "\n",
    "# 4. Class names\n",
    "class_names_path = f\"{CONFIG['model_save_dir']}/class_names.json\"\n",
    "with open(class_names_path, 'w') as f:\n",
    "    json.dump(class_names, f, indent=2)\n",
    "print(f\"‚úì Class names: {class_names_path}\")\n",
    "\n",
    "# File sizes\n",
    "keras_size = os.path.getsize(keras_path) / (1024*1024)\n",
    "tflite_size = os.path.getsize(tflite_path) / (1024*1024)\n",
    "print(f\"\\nFile sizes:\")\n",
    "print(f\"  Keras:  {keras_size:.2f} MB\")\n",
    "print(f\"  TFLite: {tflite_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üåø DR GREEN V5 - TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìä Architecture:\")\n",
    "print(f\"  Base: {CONFIG['base_model']} (FROZEN)\")\n",
    "print(f\"  Parameters: {model.count_params():,}\")\n",
    "print(f\"  Trainable: {trainable:,}\")\n",
    "\n",
    "print(f\"\\nüéØ Performance:\")\n",
    "print(f\"  Validation Accuracy: {results[1]*100:.2f}%\")\n",
    "print(f\"  Top-2 Accuracy:      {results[2]*100:.2f}%\")\n",
    "print(f\"  Overfitting Gap:     {overfitting_gap*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nüå± Per-Class:\")\n",
    "for class_name, acc in zip(class_names, per_class_accuracy):\n",
    "    emoji = \"‚úÖ\" if acc >= 0.80 else \"‚ö†Ô∏è\" if acc >= 0.70 else \"‚ùå\"\n",
    "    print(f\"  {emoji} {class_name}: {acc*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nüíæ Saved:\")\n",
    "print(f\"  ‚Ä¢ {model_name}.keras\")\n",
    "print(f\"  ‚Ä¢ {model_name}.tflite\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üöÄ READY FOR DEPLOYMENT!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Download Models (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download models from Colab\n",
    "from google.colab import files\n",
    "\n",
    "print(\"üì• Downloading models...\")\n",
    "print(\"\\nClick the links below to download:\")\n",
    "\n",
    "# Download Keras model\n",
    "files.download(keras_path)\n",
    "\n",
    "# Download TFLite model\n",
    "files.download(tflite_path)\n",
    "\n",
    "# Download metadata\n",
    "files.download(metadata_path)\n",
    "\n",
    "# Download class names\n",
    "files.download(class_names_path)\n",
    "\n",
    "print(\"\\n‚úÖ Downloads initiated!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
