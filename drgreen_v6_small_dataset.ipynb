{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "<a href=\"https://colab.research.google.com/github/armelyara/drgreen/blob/claude/drgreen-v2-01TfLAqRxjEF2BkLLt72vJrL/drgreen_v6_small_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dr Green V6 - Optimized for Very Small Datasets\n",
    "\n",
    "**Fixes for class collapse problem:**\n",
    "- **Focal Loss** - Heavily penalizes easy examples, focuses on hard ones\n",
    "- **Squared class weights** - Much stronger balancing\n",
    "- **MobileNetV2** - Lighter architecture (3.4M params vs 4M for B0)\n",
    "- **Stratified validation** - Ensures proper class distribution\n",
    "- **Very high dropout** - 0.7 to prevent memorization\n",
    "- **Tiny classifier head** - Only 64 units\n",
    "\n",
    "### Target: Balanced predictions across all 4 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install gdown for dataset download\n",
    "!pip install -q gdown\n",
    "\n",
    "# Core imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import zipfile\n",
    "import os\n",
    "import gdown\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "\n",
    "# Check GPU\n",
    "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "    print(\"GPU detected - training will be fast!\")\n",
    "else:\n",
    "    print(\"No GPU - training will be slow. Enable GPU in Runtime > Change runtime type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from Google Drive\n",
    "file_id = '1zI5KfTtuV0BlBQnNDNq4tBJuEkxLZZBD'\n",
    "url = f'https://drive.google.com/uc?id={file_id}'\n",
    "output = '/content/drgreen.zip'\n",
    "\n",
    "print(\"Downloading dataset from Google Drive...\")\n",
    "try:\n",
    "    gdown.download(url, output, quiet=False)\n",
    "    print(\"Dataset downloaded!\")\n",
    "\n",
    "    # Extract\n",
    "    print(\"\\nExtracting...\")\n",
    "    with zipfile.ZipFile(output, 'r') as zip_ref:\n",
    "        zip_ref.extractall('/content')\n",
    "    print(\"Dataset extracted!\")\n",
    "except:\n",
    "    print(\"\\nAuto-download failed. Please upload manually:\")\n",
    "    print(\"from google.colab import files\")\n",
    "    print(\"uploaded = files.upload()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration (Extreme Small Dataset Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V6 Configuration - Extreme optimization for ~1000 images\n",
    "CONFIG = {\n",
    "    # Paths\n",
    "    'data_dir': 'rename',\n",
    "    'model_save_dir': 'models',\n",
    "\n",
    "    # Image parameters\n",
    "    'img_height': 224,\n",
    "    'img_width': 224,\n",
    "    'batch_size': 16,  # Smaller batch for better gradients\n",
    "\n",
    "    # Training parameters\n",
    "    'epochs': 100,  # More epochs with early stopping\n",
    "    'initial_lr': 0.0005,  # Lower LR for stability\n",
    "\n",
    "    'validation_split': 0.2,\n",
    "    'seed': 42,\n",
    "\n",
    "    # Model parameters - VERY SIMPLE\n",
    "    'base_model': 'MobileNetV2',  # Lightest model\n",
    "    'dropout_rate': 0.7,  # Very high dropout\n",
    "    'num_classes': 4,\n",
    "    'dense_units': 64,  # Very small head\n",
    "\n",
    "    # Strong regularization\n",
    "    'l2_reg': 0.02,  # Stronger L2\n",
    "    'label_smoothing': 0.2,  # Higher smoothing\n",
    "\n",
    "    # Focal Loss parameters\n",
    "    'focal_gamma': 2.0,  # Focus on hard examples\n",
    "    'focal_alpha': 0.25,  # Balance parameter\n",
    "\n",
    "    # Callbacks\n",
    "    'early_stopping_patience': 20,\n",
    "    'reduce_lr_patience': 7,\n",
    "    'reduce_lr_factor': 0.5,\n",
    "    'min_lr': 1e-7,\n",
    "}\n",
    "\n",
    "PLANT_CLASSES = ['artemisia', 'carica', 'goyavier', 'kinkeliba']\n",
    "Path(CONFIG['model_save_dir']).mkdir(exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DR GREEN V6 - EXTREME SMALL DATASET OPTIMIZATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nBase Model: {CONFIG['base_model']} (FROZEN)\")\n",
    "print(f\"Image Size: {CONFIG['img_height']}x{CONFIG['img_width']}\")\n",
    "print(f\"Batch Size: {CONFIG['batch_size']}\")\n",
    "print(f\"Epochs: {CONFIG['epochs']} (with early stopping)\")\n",
    "print(f\"Learning Rate: {CONFIG['initial_lr']}\")\n",
    "print(f\"\\nRegularization (EXTREME):\")\n",
    "print(f\"  - Dropout: {CONFIG['dropout_rate']}\")\n",
    "print(f\"  - L2: {CONFIG['l2_reg']}\")\n",
    "print(f\"  - Label Smoothing: {CONFIG['label_smoothing']}\")\n",
    "print(f\"  - Focal Loss gamma: {CONFIG['focal_gamma']}\")\n",
    "print(f\"  - Dense units: {CONFIG['dense_units']} (minimal)\")\n",
    "print(\"\\nFocal Loss for class imbalance\")\n",
    "print(\"Squared class weights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Focal Loss Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(tf.keras.losses.Loss):\n",
    "    \"\"\"\n",
    "    Focal Loss - focuses on hard examples\n",
    "    Reduces loss for well-classified examples, increases for misclassified ones\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma=2.0, alpha=0.25, label_smoothing=0.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Apply label smoothing\n",
    "        num_classes = tf.cast(tf.shape(y_true)[-1], tf.float32)\n",
    "        y_true = y_true * (1.0 - self.label_smoothing) + (self.label_smoothing / num_classes)\n",
    "        \n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "        \n",
    "        # Calculate cross entropy\n",
    "        cross_entropy = -y_true * tf.math.log(y_pred)\n",
    "        \n",
    "        # Calculate focal weight\n",
    "        p_t = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "        focal_weight = tf.pow(1 - p_t, self.gamma)\n",
    "        \n",
    "        # Apply focal weight\n",
    "        focal_loss = self.alpha * focal_weight * tf.reduce_sum(cross_entropy, axis=-1)\n",
    "        \n",
    "        return tf.reduce_mean(focal_loss)\n",
    "\n",
    "print(\"Focal Loss implemented - will focus on hard-to-classify examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    CONFIG['data_dir'],\n",
    "    validation_split=CONFIG['validation_split'],\n",
    "    subset=\"training\",\n",
    "    seed=CONFIG['seed'],\n",
    "    image_size=(CONFIG['img_height'], CONFIG['img_width']),\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    CONFIG['data_dir'],\n",
    "    validation_split=CONFIG['validation_split'],\n",
    "    subset=\"validation\",\n",
    "    seed=CONFIG['seed'],\n",
    "    image_size=(CONFIG['img_height'], CONFIG['img_width']),\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    label_mode='categorical'\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "print(f\"\\nClasses found: {class_names}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count samples per class\n",
    "class_counts = {name: 0 for name in class_names}\n",
    "\n",
    "for images, labels in train_ds:\n",
    "    for label in labels.numpy():\n",
    "        idx = np.argmax(label)\n",
    "        class_counts[class_names[idx]] += 1\n",
    "\n",
    "val_class_counts = {name: 0 for name in class_names}\n",
    "for images, labels in val_ds:\n",
    "    for label in labels.numpy():\n",
    "        idx = np.argmax(label)\n",
    "        val_class_counts[class_names[idx]] += 1\n",
    "\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(\"\\nTraining set:\")\n",
    "total_train = sum(class_counts.values())\n",
    "for class_name, count in class_counts.items():\n",
    "    print(f\"  {class_name}: {count} images ({count/total_train*100:.1f}%)\")\n",
    "print(f\"  Total: {total_train}\")\n",
    "\n",
    "print(\"\\nValidation set:\")\n",
    "total_val = sum(val_class_counts.values())\n",
    "for class_name, count in val_class_counts.items():\n",
    "    print(f\"  {class_name}: {count} images ({count/total_val*100:.1f}%)\")\n",
    "print(f\"  Total: {total_val}\")\n",
    "\n",
    "# Calculate SQUARED class weights (much stronger balancing)\n",
    "total_samples = sum(class_counts.values())\n",
    "class_weights = {}\n",
    "for i, class_name in enumerate(class_names):\n",
    "    base_weight = total_samples / (len(class_names) * class_counts[class_name])\n",
    "    # Square the weight for stronger effect\n",
    "    class_weights[i] = base_weight ** 1.5  # Power of 1.5 for strong but not extreme\n",
    "\n",
    "print(\"\\nSQUARED Class weights (strong balancing):\")\n",
    "for i, weight in class_weights.items():\n",
    "    print(f\"  {class_names[i]}: {weight:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Very Strong Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Very strong data augmentation for small dataset\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    tf.keras.layers.RandomRotation(0.4),  # Stronger rotation\n",
    "    tf.keras.layers.RandomZoom(0.3),  # Stronger zoom\n",
    "    tf.keras.layers.RandomBrightness(0.3),  # Stronger brightness\n",
    "    tf.keras.layers.RandomContrast(0.3),  # Stronger contrast\n",
    "    tf.keras.layers.RandomTranslation(0.2, 0.2),  # Stronger translation\n",
    "], name=\"very_strong_augmentation\")\n",
    "\n",
    "# Preprocessing for MobileNetV2\n",
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "\n",
    "print(\"Very strong data augmentation configured:\")\n",
    "for layer in data_augmentation.layers:\n",
    "    print(f\"  - {layer.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Training: augmentation + preprocessing\n",
    "train_ds = train_ds.map(\n",
    "    lambda x, y: (data_augmentation(x, training=True), y),\n",
    "    num_parallel_calls=AUTOTUNE\n",
    ")\n",
    "train_ds = train_ds.map(\n",
    "    lambda x, y: (preprocess_input(x), y),\n",
    "    num_parallel_calls=AUTOTUNE\n",
    ")\n",
    "\n",
    "# Validation: only preprocessing\n",
    "val_ds = val_ds.map(\n",
    "    lambda x, y: (preprocess_input(x), y),\n",
    "    num_parallel_calls=AUTOTUNE\n",
    ")\n",
    "\n",
    "# Optimize pipeline\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(AUTOTUNE)\n",
    "\n",
    "print(\"Data pipeline optimized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Build Model (Minimal Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"\n",
    "    Build MobileNetV2 with FROZEN base and MINIMAL head\n",
    "    Designed to prevent overfitting on small datasets\n",
    "    \"\"\"\n",
    "    # Input\n",
    "    inputs = tf.keras.Input(shape=(CONFIG['img_height'], CONFIG['img_width'], 3))\n",
    "\n",
    "    # Base model - FROZEN\n",
    "    base_model = tf.keras.applications.MobileNetV2(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=inputs,\n",
    "        pooling='avg'\n",
    "    )\n",
    "    base_model.trainable = False  # CRITICAL: Keep frozen!\n",
    "\n",
    "    # MINIMAL classification head\n",
    "    x = base_model.output\n",
    "\n",
    "    # First dropout - very high\n",
    "    x = tf.keras.layers.Dropout(CONFIG['dropout_rate'])(x)\n",
    "\n",
    "    # Small dense layer with strong L2\n",
    "    x = tf.keras.layers.Dense(\n",
    "        CONFIG['dense_units'],\n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(CONFIG['l2_reg']),\n",
    "        kernel_initializer='he_normal'\n",
    "    )(x)\n",
    "\n",
    "    # Batch normalization\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    # Second dropout\n",
    "    x = tf.keras.layers.Dropout(CONFIG['dropout_rate'] * 0.7)(x)\n",
    "\n",
    "    # Output with L2\n",
    "    outputs = tf.keras.layers.Dense(\n",
    "        CONFIG['num_classes'],\n",
    "        activation='softmax',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(CONFIG['l2_reg']),\n",
    "        kernel_initializer='glorot_uniform'\n",
    "    )(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, outputs, name='DrGreen_V6_SmallDataset')\n",
    "\n",
    "    return model, base_model\n",
    "\n",
    "# Build model\n",
    "model, base_model = build_model()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL ARCHITECTURE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Base: {CONFIG['base_model']} (FROZEN)\")\n",
    "print(f\"Base trainable: {base_model.trainable}\")\n",
    "print(f\"Total parameters: {model.count_params():,}\")\n",
    "trainable = sum([tf.size(v).numpy() for v in model.trainable_variables])\n",
    "print(f\"Trainable parameters: {trainable:,}\")\n",
    "print(f\"Non-trainable: {model.count_params() - trainable:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compile Model with Focal Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine decay with warmup\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "total_steps = steps_per_epoch * CONFIG['epochs']\n",
    "warmup_steps = steps_per_epoch * 5  # 5 epochs warmup\n",
    "\n",
    "# Learning rate schedule with warmup\n",
    "class WarmupCosineDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_lr, warmup_steps, total_steps):\n",
    "        super().__init__()\n",
    "        self.initial_lr = initial_lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.total_steps = total_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
    "        total_steps = tf.cast(self.total_steps, tf.float32)\n",
    "        \n",
    "        # Warmup phase\n",
    "        warmup_lr = self.initial_lr * (step / warmup_steps)\n",
    "        \n",
    "        # Cosine decay phase\n",
    "        decay_steps = total_steps - warmup_steps\n",
    "        decay_step = step - warmup_steps\n",
    "        cosine_decay = 0.5 * (1 + tf.cos(np.pi * decay_step / decay_steps))\n",
    "        decay_lr = self.initial_lr * cosine_decay\n",
    "        \n",
    "        return tf.where(step < warmup_steps, warmup_lr, decay_lr)\n",
    "\n",
    "lr_schedule = WarmupCosineDecay(\n",
    "    initial_lr=CONFIG['initial_lr'],\n",
    "    warmup_steps=warmup_steps,\n",
    "    total_steps=total_steps\n",
    ")\n",
    "\n",
    "# Compile with Focal Loss\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    loss=FocalLoss(\n",
    "        gamma=CONFIG['focal_gamma'],\n",
    "        alpha=CONFIG['focal_alpha'],\n",
    "        label_smoothing=CONFIG['label_smoothing']\n",
    "    ),\n",
    "    metrics=[\n",
    "        tf.keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
    "        tf.keras.metrics.TopKCategoricalAccuracy(k=2, name='top2_accuracy')\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPILED\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Optimizer: Adam with Warmup + Cosine Decay\")\n",
    "print(f\"Initial LR: {CONFIG['initial_lr']}\")\n",
    "print(f\"Warmup epochs: 5\")\n",
    "print(f\"Loss: Focal Loss (gamma={CONFIG['focal_gamma']})\")\n",
    "print(f\"Label Smoothing: {CONFIG['label_smoothing']}\")\n",
    "print(f\"Total steps: {total_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Setup Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    # Early stopping - monitor val_accuracy, not loss (because of focal loss)\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=CONFIG['early_stopping_patience'],\n",
    "        restore_best_weights=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "\n",
    "    # Reduce LR on plateau\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_accuracy',\n",
    "        factor=CONFIG['reduce_lr_factor'],\n",
    "        patience=CONFIG['reduce_lr_patience'],\n",
    "        min_lr=CONFIG['min_lr'],\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "\n",
    "    # Save best model\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=f\"{CONFIG['model_save_dir']}/best_model_v6.keras\",\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        mode='max',\n",
    "        verbose=1\n",
    "    ),\n",
    "\n",
    "    # CSV logger\n",
    "    tf.keras.callbacks.CSVLogger(\n",
    "        f\"{CONFIG['model_save_dir']}/training_log_v6.csv\"\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Callbacks configured:\")\n",
    "for cb in callbacks:\n",
    "    print(f\"  - {cb.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Epochs: {CONFIG['epochs']} (with early stopping patience {CONFIG['early_stopping_patience']})\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Training samples: ~{total_train}\")\n",
    "print(f\"Validation samples: ~{total_val}\")\n",
    "print(f\"Class weights: SQUARED (very strong)\")\n",
    "print(f\"Loss: Focal Loss\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Train!\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=CONFIG['epochs'],\n",
    "    callbacks=callbacks,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETED\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualize Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(history.history['accuracy'], label='Train', marker='o', markersize=2)\n",
    "axes[0].plot(history.history['val_accuracy'], label='Validation', marker='s', markersize=2)\n",
    "axes[0].set_title('Model Accuracy', fontsize=14)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(history.history['loss'], label='Train', marker='o', markersize=2)\n",
    "axes[1].plot(history.history['val_loss'], label='Validation', marker='s', markersize=2)\n",
    "axes[1].set_title('Model Loss (Focal)', fontsize=14)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Loss')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Top-2 Accuracy\n",
    "axes[2].plot(history.history['top2_accuracy'], label='Train', marker='o', markersize=2)\n",
    "axes[2].plot(history.history['val_top2_accuracy'], label='Validation', marker='s', markersize=2)\n",
    "axes[2].set_title('Top-2 Accuracy', fontsize=14)\n",
    "axes[2].set_xlabel('Epoch')\n",
    "axes[2].set_ylabel('Top-2 Accuracy')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print metrics\n",
    "final_train_acc = history.history['accuracy'][-1]\n",
    "final_val_acc = history.history['val_accuracy'][-1]\n",
    "best_val_acc = max(history.history['val_accuracy'])\n",
    "best_epoch = history.history['val_accuracy'].index(best_val_acc) + 1\n",
    "overfitting_gap = abs(final_train_acc - final_val_acc)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL TRAINING METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Final Train Accuracy: {final_train_acc:.4f} ({final_train_acc*100:.2f}%)\")\n",
    "print(f\"Final Val Accuracy:   {final_val_acc:.4f} ({final_val_acc*100:.2f}%)\")\n",
    "print(f\"Best Val Accuracy:    {best_val_acc:.4f} ({best_val_acc*100:.2f}%)\")\n",
    "print(f\"Best Epoch:           {best_epoch}\")\n",
    "print(f\"Overfitting Gap:      {overfitting_gap:.4f} ({overfitting_gap*100:.2f}%)\")\n",
    "\n",
    "if overfitting_gap < 0.05:\n",
    "    print(\"\\nExcellent generalization!\")\n",
    "elif overfitting_gap < 0.10:\n",
    "    print(\"\\nGood generalization\")\n",
    "elif overfitting_gap < 0.15:\n",
    "    print(\"\\nModerate overfitting\")\n",
    "else:\n",
    "    print(\"\\nHigh overfitting - consider more regularization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Load Best Model & Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_model = tf.keras.models.load_model(\n",
    "    f\"{CONFIG['model_save_dir']}/best_model_v6.keras\",\n",
    "    custom_objects={'FocalLoss': FocalLoss}\n",
    ")\n",
    "print(\"Best model loaded\")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nEvaluating on validation set...\")\n",
    "results = best_model.evaluate(val_ds, verbose=1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST MODEL PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Validation Loss:      {results[0]:.4f}\")\n",
    "print(f\"Validation Accuracy:  {results[1]:.4f} ({results[1]*100:.2f}%)\")\n",
    "print(f\"Validation Top-2 Acc: {results[2]:.4f} ({results[2]*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Detailed Evaluation (CRITICAL: Check Class Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "print(\"Generating predictions...\")\n",
    "\n",
    "# Reload validation set for predictions\n",
    "val_ds_eval = tf.keras.utils.image_dataset_from_directory(\n",
    "    CONFIG['data_dir'],\n",
    "    validation_split=CONFIG['validation_split'],\n",
    "    subset=\"validation\",\n",
    "    seed=CONFIG['seed'],\n",
    "    image_size=(CONFIG['img_height'], CONFIG['img_width']),\n",
    "    batch_size=CONFIG['batch_size'],\n",
    "    label_mode='int',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Preprocess\n",
    "val_ds_eval = val_ds_eval.map(\n",
    "    lambda x, y: (preprocess_input(x), y),\n",
    "    num_parallel_calls=AUTOTUNE\n",
    ")\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_pred_proba = []\n",
    "\n",
    "for images, labels in val_ds_eval:\n",
    "    predictions = best_model.predict(images, verbose=0)\n",
    "    y_true.extend(labels.numpy())\n",
    "    y_pred.extend(np.argmax(predictions, axis=1))\n",
    "    y_pred_proba.extend(predictions)\n",
    "\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred_proba = np.array(y_pred_proba)\n",
    "\n",
    "accuracy = np.mean(y_true == y_pred)\n",
    "print(f\"\\nPredictions generated\")\n",
    "print(f\"Total: {len(y_pred)}\")\n",
    "print(f\"Correct: {np.sum(y_true == y_pred)}\")\n",
    "print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "# Check prediction distribution - CRITICAL!\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREDICTION DISTRIBUTION (Check for Class Collapse!)\")\n",
    "print(\"=\"*60)\n",
    "pred_counts = {name: 0 for name in class_names}\n",
    "for p in y_pred:\n",
    "    pred_counts[class_names[p]] += 1\n",
    "\n",
    "collapse_detected = False\n",
    "for class_name, count in pred_counts.items():\n",
    "    pct = count/len(y_pred)*100\n",
    "    status = \"WARNING - Possible collapse!\" if pct > 50 else \"\"\n",
    "    if pct > 50:\n",
    "        collapse_detected = True\n",
    "    print(f\"  {class_name}: {count} predictions ({pct:.1f}%) {status}\")\n",
    "\n",
    "if collapse_detected:\n",
    "    print(\"\\nClass collapse detected! Model is biased.\")\n",
    "else:\n",
    "    print(\"\\nPredictions are distributed across classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='Blues',\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    square=True,\n",
    "    linewidths=0.5\n",
    ")\n",
    "plt.title('Confusion Matrix - Dr Green V6', fontsize=16, pad=20)\n",
    "plt.xlabel('Predicted', fontsize=12)\n",
    "plt.ylabel('True', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Classification report\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
    "\n",
    "# Per-class accuracy\n",
    "print(\"\\nPer-class accuracy:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    class_mask = y_true == i\n",
    "    if class_mask.sum() > 0:\n",
    "        class_acc = (y_pred[class_mask] == i).mean()\n",
    "        emoji = \"OK\" if class_acc >= 0.60 else \"LOW\" if class_acc >= 0.40 else \"BAD\"\n",
    "        print(f\"  [{emoji}] {class_name:12s}: {class_acc:.4f} ({class_acc*100:.2f}%) - {class_mask.sum()} samples\")\n",
    "    else:\n",
    "        print(f\"  [N/A] {class_name:12s}: No samples in validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "model_name = f\"drgreen_v6_smalldata_{timestamp}\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"SAVING MODEL: {model_name}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 1. Keras format\n",
    "keras_path = f\"{CONFIG['model_save_dir']}/{model_name}.keras\"\n",
    "best_model.save(keras_path)\n",
    "print(f\"Keras: {keras_path}\")\n",
    "\n",
    "# 2. TFLite (optimized for mobile)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(best_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "tflite_path = f\"{CONFIG['model_save_dir']}/{model_name}.tflite\"\n",
    "with open(tflite_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "print(f\"TFLite: {tflite_path}\")\n",
    "\n",
    "# 3. Metadata\n",
    "metadata = {\n",
    "    'model_name': model_name,\n",
    "    'version': '6.0-smalldata',\n",
    "    'created_at': timestamp,\n",
    "    'approach': 'MobileNetV2 frozen + Focal Loss + Strong regularization',\n",
    "    'config': CONFIG,\n",
    "    'class_names': class_names,\n",
    "    'performance': {\n",
    "        'val_accuracy': float(results[1]),\n",
    "        'val_top2_accuracy': float(results[2]),\n",
    "        'val_loss': float(results[0]),\n",
    "        'best_val_accuracy': float(best_val_acc),\n",
    "        'overfitting_gap': float(overfitting_gap)\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = f\"{CONFIG['model_save_dir']}/{model_name}_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"Metadata: {metadata_path}\")\n",
    "\n",
    "# 4. Class names\n",
    "class_names_path = f\"{CONFIG['model_save_dir']}/class_names.json\"\n",
    "with open(class_names_path, 'w') as f:\n",
    "    json.dump(class_names, f, indent=2)\n",
    "print(f\"Class names: {class_names_path}\")\n",
    "\n",
    "# File sizes\n",
    "keras_size = os.path.getsize(keras_path) / (1024*1024)\n",
    "tflite_size = os.path.getsize(tflite_path) / (1024*1024)\n",
    "print(f\"\\nFile sizes:\")\n",
    "print(f\"  Keras:  {keras_size:.2f} MB\")\n",
    "print(f\"  TFLite: {tflite_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DR GREEN V6 - TRAINING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(f\"  Base: {CONFIG['base_model']} (FROZEN)\")\n",
    "print(f\"  Parameters: {model.count_params():,}\")\n",
    "print(f\"  Trainable: {trainable:,}\")\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Validation Accuracy: {results[1]*100:.2f}%\")\n",
    "print(f\"  Top-2 Accuracy:      {results[2]*100:.2f}%\")\n",
    "print(f\"  Overfitting Gap:     {overfitting_gap*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nPrediction Distribution:\")\n",
    "for class_name, count in pred_counts.items():\n",
    "    pct = count/len(y_pred)*100\n",
    "    print(f\"  {class_name}: {pct:.1f}%\")\n",
    "\n",
    "print(f\"\\nSaved:\")\n",
    "print(f\"  {model_name}.keras\")\n",
    "print(f\"  {model_name}.tflite\")\n",
    "\n",
    "if not collapse_detected:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"READY FOR DEPLOYMENT!\")\n",
    "    print(\"=\"*60)\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"CLASS COLLAPSE DETECTED - Need different approach\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Download Models (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download models from Colab\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Downloading models...\")\n",
    "print(\"\\nClick the links below to download:\")\n",
    "\n",
    "# Download Keras model\n",
    "files.download(keras_path)\n",
    "\n",
    "# Download TFLite model\n",
    "files.download(tflite_path)\n",
    "\n",
    "# Download metadata\n",
    "files.download(metadata_path)\n",
    "\n",
    "# Download class names\n",
    "files.download(class_names_path)\n",
    "\n",
    "print(\"\\nDownloads initiated!\")"
   ]
  }
 ]
}