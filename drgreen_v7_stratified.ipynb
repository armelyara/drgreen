{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/armelyara/drgreen/blob/claude/drgreen-v2-01TfLAqRxjEF2BkLLt72vJrL/drgreen_v7_stratified.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dr Green V7 - STRATIFIED Split (Critical Fix)\n",
        "\n",
        "**V6 Problem: All validation samples were from ONE class (kinkeliba)**\n",
        "\n",
        "This was caused by `image_dataset_from_directory` not doing stratified splits.\n",
        "\n",
        "**V7 Fix:**\n",
        "- Uses sklearn's `train_test_split` with `stratify` parameter\n",
        "- Guarantees each class is represented in both train and validation\n",
        "- Same architecture as V6 (MobileNetV2 + Focal Loss)\n",
        "\n",
        "### Target: Proper evaluation across all 4 classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup & Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install gdown for dataset download\n",
        "!pip install -q gdown\n",
        "\n",
        "# Core imports\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import json\n",
        "from datetime import datetime\n",
        "from PIL import Image\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import zipfile\n",
        "import os\n",
        "import gdown\n",
        "import glob\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
        "\n",
        "# Check GPU\n",
        "if len(tf.config.list_physical_devices('GPU')) > 0:\n",
        "    print(\"GPU detected - training will be fast!\")\n",
        "else:\n",
        "    print(\"No GPU - training will be slow. Enable GPU in Runtime > Change runtime type\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download dataset from Google Drive\n",
        "file_id = '1zI5KfTtuV0BlBQnNDNq4tBJuEkxLZZBD'\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "output = '/content/drgreen.zip'\n",
        "\n",
        "print(\"Downloading dataset from Google Drive...\")\n",
        "try:\n",
        "    gdown.download(url, output, quiet=False)\n",
        "    print(\"Dataset downloaded!\")\n",
        "\n",
        "    # Extract\n",
        "    print(\"\\nExtracting...\")\n",
        "    with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content')\n",
        "    print(\"Dataset extracted!\")\n",
        "except:\n",
        "    print(\"\\nAuto-download failed. Please upload manually:\")\n",
        "    print(\"from google.colab import files\")\n",
        "    print(\"uploaded = files.upload()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# V7 Configuration - Same as V6 but with stratified split\n",
        "CONFIG = {\n",
        "    # Paths\n",
        "    'data_dir': 'rename',\n",
        "    'model_save_dir': 'models',\n",
        "\n",
        "    # Image parameters\n",
        "    'img_height': 224,\n",
        "    'img_width': 224,\n",
        "    'batch_size': 16,\n",
        "\n",
        "    # Training parameters\n",
        "    'epochs': 100,\n",
        "    'initial_lr': 0.0005,\n",
        "\n",
        "    'validation_split': 0.2,\n",
        "    'seed': 42,\n",
        "\n",
        "    # Model parameters\n",
        "    'base_model': 'MobileNetV2',\n",
        "    'dropout_rate': 0.6,  # Slightly lower than V6\n",
        "    'num_classes': 4,\n",
        "    'dense_units': 64,\n",
        "\n",
        "    # Regularization\n",
        "    'l2_reg': 0.02,\n",
        "    'label_smoothing': 0.15,\n",
        "\n",
        "    # Focal Loss parameters\n",
        "    'focal_gamma': 2.0,\n",
        "    'focal_alpha': 0.25,\n",
        "\n",
        "    # Callbacks\n",
        "    'early_stopping_patience': 15,\n",
        "    'reduce_lr_patience': 5,\n",
        "    'reduce_lr_factor': 0.5,\n",
        "    'min_lr': 1e-7,\n",
        "}\n",
        "\n",
        "PLANT_CLASSES = ['artemisia', 'carica', 'goyavier', 'kinkeliba']\n",
        "Path(CONFIG['model_save_dir']).mkdir(exist_ok=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DR GREEN V7 - STRATIFIED SPLIT\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nBase Model: {CONFIG['base_model']} (FROZEN)\")\n",
        "print(f\"Image Size: {CONFIG['img_height']}x{CONFIG['img_width']}\")\n",
        "print(f\"Batch Size: {CONFIG['batch_size']}\")\n",
        "print(f\"\\nCRITICAL FIX: Using sklearn stratified split\")\n",
        "print(\"This ensures all classes are in both train and validation!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Focal Loss Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FocalLoss(tf.keras.losses.Loss):\n",
        "    def __init__(self, gamma=2.0, alpha=0.25, label_smoothing=0.0, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.label_smoothing = label_smoothing\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "        num_classes = tf.cast(tf.shape(y_true)[-1], tf.float32)\n",
        "        y_true = y_true * (1.0 - self.label_smoothing) + (self.label_smoothing / num_classes)\n",
        "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
        "        cross_entropy = -y_true * tf.math.log(y_pred)\n",
        "        p_t = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
        "        focal_weight = tf.pow(1 - p_t, self.gamma)\n",
        "        focal_loss = self.alpha * focal_weight * tf.reduce_sum(cross_entropy, axis=-1)\n",
        "        return tf.reduce_mean(focal_loss)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            'gamma': self.gamma,\n",
        "            'alpha': self.alpha,\n",
        "            'label_smoothing': self.label_smoothing\n",
        "        })\n",
        "        return config\n",
        "\n",
        "print(\"Focal Loss implemented\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. STRATIFIED Dataset Loading (Critical Fix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all image paths and labels\n",
        "data_dir = Path(CONFIG['data_dir'])\n",
        "class_names = sorted([d.name for d in data_dir.iterdir() if d.is_dir()])\n",
        "print(f\"Classes found: {class_names}\")\n",
        "\n",
        "# Collect all image paths and labels\n",
        "all_image_paths = []\n",
        "all_labels = []\n",
        "\n",
        "for class_idx, class_name in enumerate(class_names):\n",
        "    class_dir = data_dir / class_name\n",
        "    for ext in ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']:\n",
        "        for img_path in class_dir.glob(ext):\n",
        "            all_image_paths.append(str(img_path))\n",
        "            all_labels.append(class_idx)\n",
        "\n",
        "all_image_paths = np.array(all_image_paths)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "print(f\"\\nTotal images: {len(all_image_paths)}\")\n",
        "for i, name in enumerate(class_names):\n",
        "    count = (all_labels == i).sum()\n",
        "    print(f\"  {name}: {count} images\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# STRATIFIED SPLIT using sklearn\n",
        "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "    all_image_paths,\n",
        "    all_labels,\n",
        "    test_size=CONFIG['validation_split'],\n",
        "    random_state=CONFIG['seed'],\n",
        "    stratify=all_labels  # CRITICAL: This ensures balanced split!\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STRATIFIED SPLIT RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nTraining set: {len(train_paths)} images\")\n",
        "for i, name in enumerate(class_names):\n",
        "    count = (train_labels == i).sum()\n",
        "    pct = count / len(train_labels) * 100\n",
        "    print(f\"  {name}: {count} images ({pct:.1f}%)\")\n",
        "\n",
        "print(f\"\\nValidation set: {len(val_paths)} images\")\n",
        "for i, name in enumerate(class_names):\n",
        "    count = (val_labels == i).sum()\n",
        "    pct = count / len(val_labels) * 100\n",
        "    print(f\"  {name}: {count} images ({pct:.1f}%)\")\n",
        "\n",
        "print(\"\\nAll classes are now properly represented in validation!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create TensorFlow datasets from paths\n",
        "def load_and_preprocess_image(path, label):\n",
        "    # Load image\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, [CONFIG['img_height'], CONFIG['img_width']])\n",
        "    return img, label\n",
        "\n",
        "# Create datasets\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
        "train_ds = train_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
        "val_ds = val_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Convert labels to one-hot\n",
        "train_ds = train_ds.map(lambda x, y: (x, tf.one_hot(y, CONFIG['num_classes'])))\n",
        "val_ds = val_ds.map(lambda x, y: (x, tf.one_hot(y, CONFIG['num_classes'])))\n",
        "\n",
        "print(\"Datasets created from stratified split\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Augmentation & Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data augmentation\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
        "    tf.keras.layers.RandomRotation(0.3),\n",
        "    tf.keras.layers.RandomZoom(0.2),\n",
        "    tf.keras.layers.RandomBrightness(0.2),\n",
        "    tf.keras.layers.RandomContrast(0.2),\n",
        "    tf.keras.layers.RandomTranslation(0.15, 0.15),\n",
        "], name=\"data_augmentation\")\n",
        "\n",
        "# Preprocessing for MobileNetV2\n",
        "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
        "\n",
        "# Apply augmentation and preprocessing\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.map(\n",
        "    lambda x, y: (data_augmentation(x, training=True), y),\n",
        "    num_parallel_calls=AUTOTUNE\n",
        ")\n",
        "train_ds = train_ds.map(\n",
        "    lambda x, y: (preprocess_input(x), y),\n",
        "    num_parallel_calls=AUTOTUNE\n",
        ")\n",
        "\n",
        "val_ds = val_ds.map(\n",
        "    lambda x, y: (preprocess_input(x), y),\n",
        "    num_parallel_calls=AUTOTUNE\n",
        ")\n",
        "\n",
        "# Batch and optimize\n",
        "train_ds = train_ds.shuffle(1000).batch(CONFIG['batch_size']).prefetch(AUTOTUNE)\n",
        "val_ds = val_ds.batch(CONFIG['batch_size']).prefetch(AUTOTUNE)\n",
        "\n",
        "print(\"Data pipeline configured\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate class weights\n",
        "total_train = len(train_labels)\n",
        "class_weights = {}\n",
        "for i, class_name in enumerate(class_names):\n",
        "    count = (train_labels == i).sum()\n",
        "    base_weight = total_train / (len(class_names) * count)\n",
        "    class_weights[i] = base_weight ** 1.3  # Moderate power\n",
        "\n",
        "print(\"Class weights:\")\n",
        "for i, weight in class_weights.items():\n",
        "    print(f\"  {class_names[i]}: {weight:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_model():\n",
        "    inputs = tf.keras.Input(shape=(CONFIG['img_height'], CONFIG['img_width'], 3))\n",
        "\n",
        "    base_model = tf.keras.applications.MobileNetV2(\n",
        "        include_top=False,\n",
        "        weights='imagenet',\n",
        "        input_tensor=inputs,\n",
        "        pooling='avg'\n",
        "    )\n",
        "    base_model.trainable = False\n",
        "\n",
        "    x = base_model.output\n",
        "    x = tf.keras.layers.Dropout(CONFIG['dropout_rate'])(x)\n",
        "    x = tf.keras.layers.Dense(\n",
        "        CONFIG['dense_units'],\n",
        "        activation='relu',\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(CONFIG['l2_reg']),\n",
        "        kernel_initializer='he_normal'\n",
        "    )(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(CONFIG['dropout_rate'] * 0.5)(x)\n",
        "    outputs = tf.keras.layers.Dense(\n",
        "        CONFIG['num_classes'],\n",
        "        activation='softmax',\n",
        "        kernel_regularizer=tf.keras.regularizers.l2(CONFIG['l2_reg'])\n",
        "    )(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs, outputs, name='DrGreen_V7_Stratified')\n",
        "    return model, base_model\n",
        "\n",
        "model, base_model = build_model()\n",
        "\n",
        "print(\"\\nModel Architecture:\")\n",
        "print(f\"  Base: {CONFIG['base_model']} (FROZEN)\")\n",
        "print(f\"  Total parameters: {model.count_params():,}\")\n",
        "trainable = sum([tf.size(v).numpy() for v in model.trainable_variables])\n",
        "print(f\"  Trainable parameters: {trainable:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Compile & Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Learning rate schedule\n",
        "steps_per_epoch = len(train_labels) // CONFIG['batch_size']\n",
        "total_steps = steps_per_epoch * CONFIG['epochs']\n",
        "\n",
        "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
        "    initial_learning_rate=CONFIG['initial_lr'],\n",
        "    decay_steps=total_steps,\n",
        "    alpha=0.01\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
        "    loss=FocalLoss(\n",
        "        gamma=CONFIG['focal_gamma'],\n",
        "        alpha=CONFIG['focal_alpha'],\n",
        "        label_smoothing=CONFIG['label_smoothing']\n",
        "    ),\n",
        "    metrics=[\n",
        "        tf.keras.metrics.CategoricalAccuracy(name='accuracy'),\n",
        "        tf.keras.metrics.TopKCategoricalAccuracy(k=2, name='top2_accuracy')\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(\"Model compiled with Focal Loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_accuracy',\n",
        "        patience=CONFIG['early_stopping_patience'],\n",
        "        restore_best_weights=True,\n",
        "        mode='max',\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(\n",
        "        monitor='val_accuracy',\n",
        "        factor=CONFIG['reduce_lr_factor'],\n",
        "        patience=CONFIG['reduce_lr_patience'],\n",
        "        min_lr=CONFIG['min_lr'],\n",
        "        mode='max',\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=f\"{CONFIG['model_save_dir']}/best_model_v7.keras\",\n",
        "        monitor='val_accuracy',\n",
        "        save_best_only=True,\n",
        "        mode='max',\n",
        "        verbose=1\n",
        "    ),\n",
        "    tf.keras.callbacks.CSVLogger(\n",
        "        f\"{CONFIG['model_save_dir']}/training_log_v7.csv\"\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"Callbacks configured\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Training samples: {len(train_labels)}\")\n",
        "print(f\"Validation samples: {len(val_labels)}\")\n",
        "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
        "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=CONFIG['epochs'],\n",
        "    callbacks=callbacks,\n",
        "    class_weight=class_weights,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nTraining completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "axes[0].plot(history.history['accuracy'], label='Train')\n",
        "axes[0].plot(history.history['val_accuracy'], label='Validation')\n",
        "axes[0].set_title('Model Accuracy')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1].plot(history.history['loss'], label='Train')\n",
        "axes[1].plot(history.history['val_loss'], label='Validation')\n",
        "axes[1].set_title('Model Loss')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Loss')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "axes[2].plot(history.history['top2_accuracy'], label='Train')\n",
        "axes[2].plot(history.history['val_top2_accuracy'], label='Validation')\n",
        "axes[2].set_title('Top-2 Accuracy')\n",
        "axes[2].set_xlabel('Epoch')\n",
        "axes[2].set_ylabel('Top-2 Accuracy')\n",
        "axes[2].legend()\n",
        "axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final metrics\n",
        "final_train_acc = history.history['accuracy'][-1]\n",
        "final_val_acc = history.history['val_accuracy'][-1]\n",
        "best_val_acc = max(history.history['val_accuracy'])\n",
        "best_epoch = history.history['val_accuracy'].index(best_val_acc) + 1\n",
        "overfitting_gap = abs(final_train_acc - final_val_acc)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL METRICS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Final Train Accuracy: {final_train_acc*100:.2f}%\")\n",
        "print(f\"Final Val Accuracy:   {final_val_acc*100:.2f}%\")\n",
        "print(f\"Best Val Accuracy:    {best_val_acc*100:.2f}%\")\n",
        "print(f\"Best Epoch:           {best_epoch}\")\n",
        "print(f\"Overfitting Gap:      {overfitting_gap*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Detailed Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load best model\n",
        "best_model = tf.keras.models.load_model(\n",
        "    f\"{CONFIG['model_save_dir']}/best_model_v7.keras\",\n",
        "    custom_objects={'FocalLoss': FocalLoss}\n",
        ")\n",
        "print(\"Best model loaded\")\n",
        "\n",
        "# Evaluate\n",
        "results = best_model.evaluate(val_ds, verbose=1)\n",
        "print(f\"\\nValidation Accuracy: {results[1]*100:.2f}%\")\n",
        "print(f\"Top-2 Accuracy: {results[2]*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get predictions\n",
        "print(\"Generating predictions...\")\n",
        "\n",
        "# Create evaluation dataset without augmentation\n",
        "val_ds_eval = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
        "val_ds_eval = val_ds_eval.map(load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n",
        "val_ds_eval = val_ds_eval.map(\n",
        "    lambda x, y: (preprocess_input(x), y),\n",
        "    num_parallel_calls=AUTOTUNE\n",
        ")\n",
        "val_ds_eval = val_ds_eval.batch(CONFIG['batch_size'])\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for images, labels in val_ds_eval:\n",
        "    predictions = best_model.predict(images, verbose=0)\n",
        "    y_true.extend(labels.numpy())\n",
        "    y_pred.extend(np.argmax(predictions, axis=1))\n",
        "\n",
        "y_true = np.array(y_true)\n",
        "y_pred = np.array(y_pred)\n",
        "\n",
        "accuracy = np.mean(y_true == y_pred)\n",
        "print(f\"\\nAccuracy: {accuracy*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prediction distribution\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PREDICTION DISTRIBUTION\")\n",
        "print(\"=\"*60)\n",
        "pred_counts = {name: 0 for name in class_names}\n",
        "for p in y_pred:\n",
        "    pred_counts[class_names[p]] += 1\n",
        "\n",
        "collapse_detected = False\n",
        "for class_name, count in pred_counts.items():\n",
        "    pct = count/len(y_pred)*100\n",
        "    if pct > 50:\n",
        "        collapse_detected = True\n",
        "        print(f\"  {class_name}: {count} ({pct:.1f}%) - WARNING\")\n",
        "    else:\n",
        "        print(f\"  {class_name}: {count} ({pct:.1f}%)\")\n",
        "\n",
        "if collapse_detected:\n",
        "    print(\"\\nClass collapse detected!\")\n",
        "else:\n",
        "    print(\"\\nPredictions are balanced!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    cm,\n",
        "    annot=True,\n",
        "    fmt='d',\n",
        "    cmap='Blues',\n",
        "    xticklabels=class_names,\n",
        "    yticklabels=class_names,\n",
        "    square=True\n",
        ")\n",
        "plt.title('Confusion Matrix - Dr Green V7 (Stratified)', fontsize=14)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Classification report\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CLASSIFICATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n",
        "\n",
        "# Per-class accuracy\n",
        "print(\"\\nPer-class accuracy:\")\n",
        "for i, class_name in enumerate(class_names):\n",
        "    class_mask = y_true == i\n",
        "    if class_mask.sum() > 0:\n",
        "        class_acc = (y_pred[class_mask] == i).mean()\n",
        "        status = \"OK\" if class_acc >= 0.60 else \"LOW\" if class_acc >= 0.40 else \"BAD\"\n",
        "        print(f\"  [{status}] {class_name}: {class_acc*100:.2f}% ({class_mask.sum()} samples)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "model_name = f\"drgreen_v7_stratified_{timestamp}\"\n",
        "\n",
        "print(f\"Saving model: {model_name}\")\n",
        "\n",
        "# Keras format\n",
        "keras_path = f\"{CONFIG['model_save_dir']}/{model_name}.keras\"\n",
        "best_model.save(keras_path)\n",
        "print(f\"  Keras: {keras_path}\")\n",
        "\n",
        "# TFLite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(best_model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "tflite_path = f\"{CONFIG['model_save_dir']}/{model_name}.tflite\"\n",
        "with open(tflite_path, 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "print(f\"  TFLite: {tflite_path}\")\n",
        "\n",
        "# Metadata\n",
        "metadata = {\n",
        "    'model_name': model_name,\n",
        "    'version': '7.0-stratified',\n",
        "    'created_at': timestamp,\n",
        "    'class_names': class_names,\n",
        "    'performance': {\n",
        "        'val_accuracy': float(results[1]),\n",
        "        'val_top2_accuracy': float(results[2]),\n",
        "        'best_val_accuracy': float(best_val_acc),\n",
        "        'overfitting_gap': float(overfitting_gap)\n",
        "    }\n",
        "}\n",
        "\n",
        "metadata_path = f\"{CONFIG['model_save_dir']}/{model_name}_metadata.json\"\n",
        "with open(metadata_path, 'w') as f:\n",
        "    json.dump(metadata, f, indent=2)\n",
        "print(f\"  Metadata: {metadata_path}\")\n",
        "\n",
        "# File sizes\n",
        "keras_size = os.path.getsize(keras_path) / (1024*1024)\n",
        "tflite_size = os.path.getsize(tflite_path) / (1024*1024)\n",
        "print(f\"\\nFile sizes: Keras={keras_size:.2f}MB, TFLite={tflite_size:.2f}MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Final Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DR GREEN V7 - TRAINING COMPLETE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"\\nArchitecture: {CONFIG['base_model']} (frozen)\")\n",
        "print(f\"Parameters: {model.count_params():,} ({trainable:,} trainable)\")\n",
        "\n",
        "print(f\"\\nPerformance:\")\n",
        "print(f\"  Validation Accuracy: {results[1]*100:.2f}%\")\n",
        "print(f\"  Top-2 Accuracy: {results[2]*100:.2f}%\")\n",
        "print(f\"  Overfitting Gap: {overfitting_gap*100:.2f}%\")\n",
        "\n",
        "print(f\"\\nPrediction Distribution:\")\n",
        "for class_name, count in pred_counts.items():\n",
        "    pct = count/len(y_pred)*100\n",
        "    print(f\"  {class_name}: {pct:.1f}%\")\n",
        "\n",
        "print(f\"\\nSaved: {model_name}.keras, {model_name}.tflite\")\n",
        "\n",
        "if not collapse_detected:\n",
        "    print(\"\\nREADY FOR DEPLOYMENT!\")\n",
        "else:\n",
        "    print(\"\\nNeed further optimization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Download Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "print(\"Downloading models...\")\n",
        "files.download(keras_path)\n",
        "files.download(tflite_path)\n",
        "files.download(metadata_path)\n",
        "print(\"Downloads initiated!\")"
      ]
    }
  ]
}
